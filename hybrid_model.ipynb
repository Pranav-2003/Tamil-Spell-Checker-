{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import copy\n",
    "from tamil import utf8\n",
    "import time\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "with open('/kaggle/input/clean-test-data/data.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.readlines()\n",
    "\n",
    "def remove_parenthesis_text(lines):\n",
    "    pattern = r'\\(.?\\)|<\\/?doc.?>|[^ \\u0B80-\\u0BFF]'\n",
    "    cleaned_lines = [re.sub(pattern, '', line).strip() for line in lines]\n",
    "    return cleaned_lines\n",
    "\n",
    "def clean_line(line):\n",
    "    return re.sub(r'[^஀-௿\\s]', '', line)\n",
    "\n",
    "text = remove_parenthesis_text(text)\n",
    "\n",
    "text = [clean_line(line) for line in text]\n",
    "\n",
    "def generate_ngrams(words, n):\n",
    "    \"\"\"Generate n-grams from a list of words.\"\"\"\n",
    "    return [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "words=[]\n",
    "unigrams = Counter()\n",
    "bigrams = Counter()\n",
    "trigrams = Counter()\n",
    "for line in text:\n",
    "    words_in_line = line.strip().split()\n",
    "    unigrams.update(words_in_line)  # Unigrams\n",
    "    bigrams.update(generate_ngrams(words_in_line, 2))  # Bigrams\n",
    "    trigrams.update(generate_ngrams(words_in_line, 3))  # Trigrams\n",
    "    words.extend(line.split(\" \"))\n",
    "\n",
    "words=[word for word in words if word!='']\n",
    "\n",
    "vocab=set(words)\n",
    "print(\"unique words in vocab = \",len(vocab))\n",
    "word_count_dict = {}  \n",
    "word_count_dict = Counter(words)\n",
    "probs = {} \n",
    "m = sum(word_count_dict.values())\n",
    "for k, v in word_count_dict.items():\n",
    "    probs[k] = v / m\n",
    "\n",
    "def delete_letter(word):\n",
    "    letters = utf8.get_letters(word)\n",
    "    \n",
    "    delete_l = []\n",
    "    if len(letters)<3:\n",
    "        return word\n",
    "    \n",
    "    for i in range(1,len(letters)):\n",
    "        new_word=''.join(letters[:i])\n",
    "        if len(letters)>i+1:\n",
    "            new_word+=''.join(letters[i+1:])\n",
    "        delete_l.append(new_word)\n",
    "\n",
    "    return delete_l\n",
    "\n",
    "def insert_letter(word):\n",
    "    letters = utf8.get_letters(word)\n",
    "    insert_l=[]\n",
    "        \n",
    "    for l in ['க', 'ச', 'ட', 'த', 'ப', 'ற',  \n",
    "                'ம', 'ய', 'ர', 'வ', 'ஜ', 'ஸ']:\n",
    "        x=copy.deepcopy(letters)\n",
    "        x.append(l+'்')\n",
    "        insert_l.append(''.join(x))\n",
    "    \n",
    "    return insert_l\n",
    "\n",
    "def transpose_letter(word):\n",
    "    letters = utf8.get_letters(word)\n",
    "    transpose_l=[]\n",
    "    for i in range(1,len(letters)-1):\n",
    "        x=copy.deepcopy(letters)\n",
    "        x[i],x[i+1]=x[i+1],x[i]\n",
    "        transpose_l.append(''.join(x))\n",
    "    \n",
    "    return transpose_l    \n",
    "\n",
    "def substitute_letter(word):\n",
    "    letters = utf8.get_letters(word)\n",
    "    target_chars = [['ர','ற'],['ல','ள','ழ'],['ன','ண','ந'],['ங','ஞ']]\n",
    "    substitute_l=[]\n",
    "    for i in range(0,len(letters)):\n",
    "        \n",
    "        for l in target_chars:\n",
    "            if letters[i][0] in l:\n",
    "                for char in [char for char in l if char != letters[i][0]]:\n",
    "                    x=copy.deepcopy(letters)\n",
    "                    if len(x[i])==2:\n",
    "                        x[i]=char+x[i][1]\n",
    "                    else:\n",
    "                        x[i]=char\n",
    "                    substitute_l.append(''.join(x))\n",
    "    \n",
    "    target_chars = [[ 'ி', 'ீ'],['','ா','ை'],['ு', 'ூ'],['ெ', 'ே'],['ொ', 'ோ']]\n",
    "    for i in range(0,len(letters)):\n",
    "        \n",
    "        for l in target_chars:\n",
    "            if len(letters[i])==1 and letters[i] in ['க', 'ச', 'ட', 'த', 'ப', 'ற', 'ஞ', 'ங', 'ண', 'ந', \n",
    "                   'ம','ன', 'ய', 'ர', 'ல', 'வ', 'ழ', 'ள', 'ஜ', 'ஷ', 'ஸ', 'ஹ']:\n",
    "                for char in ['ா','ை']:\n",
    "                    x=copy.deepcopy(letters)\n",
    "                    x[i]+=char\n",
    "                    substitute_l.append(''.join(x))\n",
    "            elif len(letters[i])==2 and letters[i][1] in l:\n",
    "                for char in [char for char in l if char != letters[i][1]]:\n",
    "                    x=copy.deepcopy(letters)\n",
    "                    x[i]=x[i][0]+char\n",
    "                    substitute_l.append(''.join(x))\n",
    "    \n",
    "    return substitute_l\n",
    "\n",
    "def edit_one_letter(word):\n",
    "    edit_one_set = set()\n",
    "    \n",
    "    edit_one_set.update(delete_letter(word))\n",
    "    edit_one_set.update(transpose_letter(word))\n",
    "    edit_one_set.update(substitute_letter(word))\n",
    "    edit_one_set.update(insert_letter(word))\n",
    "\n",
    "    return edit_one_set\n",
    "\n",
    "words_outof_context=0\n",
    "\n",
    "# Compute Context Probability (Using Bigrams and Trigrams)\n",
    "def ngram_probability(word, prev_word, prev_prev_word):\n",
    "    \"\"\"Compute probability using unigrams, bigrams, and trigrams.\"\"\"\n",
    "    unigram_prob = unigrams.get(word, 0) / sum(unigrams.values())\n",
    "    bigram_prob = bigrams.get((prev_word, word), 0) / unigrams.get(prev_word, 1)\n",
    "    trigram_prob = trigrams.get((prev_prev_word, prev_word, word), 0) / bigrams.get((prev_prev_word, prev_word), 1)\n",
    "\n",
    "    # Weighted probability combination\n",
    "    return 0.5 * trigram_prob + 0.4 * bigram_prob + 0.1 * unigram_prob\n",
    " \n",
    "\n",
    "def get_corrections(word, probs, vocab,  prev_word, prev_prev_word,n=3):\n",
    "    \n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    \n",
    "    if word in vocab and ngram_probability(word,prev_word,prev_prev_word)>0.00001:\n",
    "        return 'Nil',{word}\n",
    "        \n",
    "    one_error_set=edit_one_letter(word)\n",
    "    suggestions=one_error_set.intersection(vocab)\n",
    "    one_error=list(one_error_set)\n",
    "    letters=utf8.get_letters(word)\n",
    "    \n",
    "    for i in one_error:\n",
    "        suggestions=suggestions.union(edit_one_letter(i).intersection(vocab))\n",
    "    \n",
    "    suggestions=list(suggestions)\n",
    "    suggestions.append(word)\n",
    "    n_best_list_tuple = sorted(\n",
    "    [(word, probs[word]) if word in probs else (word, 0) for word in suggestions]\n",
    ",\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "    n_best_list=[word for (word,probs) in n_best_list_tuple]\n",
    "    n_best_list = list(dict.fromkeys(n_best_list))[:10]\n",
    "    n_best_list = sorted(n_best_list, key=lambda w: ngram_probability(w, prev_word, prev_prev_word), reverse=True) \n",
    "    n_best_list = n_best_list[:3]\n",
    "    n_best=set(n_best_list)\n",
    "    return n_best_list, n_best \n",
    "\n",
    "corrected=0\n",
    "missed=0\n",
    "file_no=1\n",
    "with open('/kaggle/input/clean-test-data/error_details_5.txt', 'r',encoding='utf-8') as file:\n",
    "    values = file.readlines()\n",
    "\n",
    "no_error_list = [int(value.strip()) for value in values[0].split(',')]\n",
    "no_error_index = set(no_error_list)\n",
    "\n",
    "single_error_list = [int(value.strip()) for value in values[1].split(',')]\n",
    "single_error_index = set(single_error_list)\n",
    "\n",
    "double_error_list = [int(value.strip()) for value in values[2].split(',')]\n",
    "double_error_index = set(double_error_list)\n",
    "\n",
    "with open('/kaggle/input/clean-test-data/error_file_5.txt', 'r', encoding='utf-8') as file:\n",
    "    test_data = file.readlines()\n",
    "    \n",
    "\n",
    "test_data = remove_parenthesis_text(test_data)\n",
    "test_data = [clean_line(line) for line in test_data]\n",
    "for i in range(len(test_data)):\n",
    "    test_data[i]=test_data[i].split(\" \")\n",
    "\n",
    "test_data = [[word for word in line if word] for line in test_data]\n",
    "word_cnt=0\n",
    "\n",
    "with open(\"/kaggle/input/clean-test-data/clean_test_data.txt\", 'r',encoding='utf-8') as file:\n",
    "    test_data_correct=file.readlines()\n",
    "    test_data_correct = remove_parenthesis_text(test_data_correct)\n",
    "    test_data_correct=test_data_correct[1200:1500]\n",
    "\n",
    "    test_data_correct = [clean_line(line) for line in test_data_correct]\n",
    "x=0\n",
    "for i in range(len(test_data_correct)):\n",
    "    test_data_correct[i]=test_data_correct[i].split(\" \")\n",
    "    if (len(test_data[i])==len(test_data_correct[i])):\n",
    "        x+=1\n",
    "\n",
    "test_data_correct = [[word for word in line if word] for line in test_data_correct]\n",
    "\n",
    "not_in_vocab=0    \n",
    "correctly_predicted_no_error=0\n",
    "correctly_predicted_single_error=0\n",
    "correctly_predicted_top_suggestion=0\n",
    "correctly_predicted_double_error=0\n",
    "correctly_given_firstoption_no_error=0\n",
    "\n",
    "punctuations = {'.',',','\"',\"'\"}\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    prev_word=\"<s>\"\n",
    "    prev_prev_word=\"<s>\"\n",
    "    for j in range(len(test_data[i])):\n",
    "        if test_data[i][j] in punctuations:\n",
    "            continue\n",
    "        \n",
    "        chk_list,chk = get_corrections(test_data[i][j],probs,vocab,prev_word,prev_prev_word)\n",
    "        if len(chk)>0 and (j+1)!=len(test_data[i]) and test_data[i][j+1][0] in [\"க\", \"ச\", \"ட\", \"த\",\"ப\",\"ற\"]:\n",
    "            if(chk_list=='Nil'):\n",
    "                chk_list = list(chk)\n",
    "            expected_sandhi=test_data[i][j+1][0]+'்'\n",
    "            length=len(chk)\n",
    "            for k in range(length):\n",
    "                if chk_list[k][-1]!='்':\n",
    "                    chk_list.append(chk_list[k]+expected_sandhi)\n",
    "                    chk.add(chk_list[k]+expected_sandhi)\n",
    "        if (word_cnt in no_error_index):\n",
    "            if chk_list==\"Nil\" or test_data_correct[i][j] in chk:\n",
    "                correctly_predicted_no_error+=1\n",
    "            if chk_list==\"Nil\" or (len(chk_list)>0 and chk_list[0]==test_data_correct[i][j]):\n",
    "                correctly_given_firstoption_no_error+=1\n",
    "\n",
    "        elif (word_cnt in single_error_index):\n",
    "            \n",
    "                if test_data_correct[i][j] in chk:\n",
    "                    correctly_predicted_single_error+=1\n",
    "                if chk_list[0]==test_data_correct[i][j]:\n",
    "                    correctly_predicted_top_suggestion+=1\n",
    "            \n",
    "        elif (word_cnt in double_error_index):\n",
    "            \n",
    "                if test_data_correct[i][j] in chk:\n",
    "                    correctly_predicted_double_error+=1\n",
    "                if chk_list[0]==test_data_correct[i][j]:\n",
    "                    correctly_predicted_top_suggestion+=1\n",
    "        word_cnt+=1\n",
    "\n",
    "        if(test_data_correct[i][j] not in vocab):\n",
    "            not_in_vocab+=1\n",
    "\n",
    "        if(word_cnt%1000 == 0):\n",
    "            print(word_cnt)\n",
    "        \n",
    "        prev_prev_word=prev_word\n",
    "        if(chk_list==\"Nil\" or chk_list==[]):\n",
    "            prev_word=test_data[i][j]\n",
    "        else:\n",
    "            prev_word=chk_list[0]\n",
    "\n",
    "print(\"Correct No error prediction: \", str(correctly_predicted_no_error))\n",
    "print(\"Correct Single error prediction: \", str(correctly_predicted_single_error))\n",
    "print(\"Correct Double error prediction: \", str(correctly_predicted_double_error))\n",
    "print(\"Total no error: \", str(len(no_error_list)))\n",
    "print(\"Total single error: \", str(len(single_error_list)))\n",
    "print(\"Total double error: \", str(len(double_error_list)))\n",
    "print(\"Total Accuracy: \", str((correctly_predicted_no_error+correctly_predicted_single_error+correctly_predicted_double_error)/(len(no_error_list)+len(single_error_list)+len(double_error_list))))\n",
    "print(\"Accuracy among errors: \", str((correctly_predicted_single_error+correctly_predicted_double_error)/(len(single_error_list)+len(double_error_list))))\n",
    "print(\"Total top suggestion accuracy: \", str((correctly_predicted_top_suggestion+correctly_given_firstoption_no_error)/(len(no_error_list)+len(single_error_list)+len(double_error_list))))\n",
    "print(\"Top suggestion accuracy among errors: \", str(correctly_predicted_top_suggestion/(len(single_error_list)+len(double_error_list))))\n",
    "\n",
    "finish = time.perf_counter()\n",
    "\n",
    "print(\"Time =\",round(finish-start,2),\"sec\")\n",
    "# print(\"Words out of correction = \",words_outof_context)\n",
    "print(\"Not in Vocab =\",not_in_vocab, not_in_vocab*100/(len(single_error_list)+len(double_error_list)+len(no_error_list)),\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import MT5Tokenizer,MT5ForConditionalGeneration, logging\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os, math, string\n",
    "import re, Levenshtein\n",
    "from collections import Counter\n",
    "import copy\n",
    "from tamil import utf8\n",
    "import time\n",
    "logging.set_verbosity_error()\n",
    "import fasttext.util\n",
    "os.environ['FASTTEXT_VERBOSE'] = '0'\n",
    "fasttext.util.download_model('ta', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.ta.300.bin')\n",
    "\n",
    "def get_fasttext_suggestions(word, top_n=15):\n",
    "    try:\n",
    "        suggestions = {}  \n",
    "        \n",
    "        for suggestion in ft.get_nearest_neighbors(word, k=top_n):  \n",
    "            suggested_word = suggestion[1]    \n",
    "            dist = Levenshtein.distance(word, suggested_word)  \n",
    "    \n",
    "            if dist <= 4:  # Only keep words within max distance\n",
    "                suggestions[suggested_word] = dist \n",
    "                \n",
    "        return suggestions\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "model_name = \"google/mt5-base\"  \n",
    "qa_model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "qa_model.to(device)\n",
    "\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "\n",
    "with open('/kaggle/input/clean-test-data/data.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.readlines()\n",
    "\n",
    "def remove_parenthesis_text(lines):\n",
    "    pattern = r'\\(.?\\)|<\\/?doc.?>|[^ \\u0B80-\\u0BFF]'\n",
    "    cleaned_lines = [re.sub(pattern, '', line).strip() for line in lines]\n",
    "    return cleaned_lines\n",
    "\n",
    "def clean_line(line):\n",
    "    return re.sub(r'[^஀-௿\\s]', '', line)\n",
    "\n",
    "text = remove_parenthesis_text(text)\n",
    "\n",
    "text = [clean_line(line) for line in text]\n",
    "\n",
    "def generate_ngrams(words, n):\n",
    "    \"\"\"Generate n-grams from a list of words.\"\"\"\n",
    "    return [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "words=[]\n",
    "unigrams = Counter()\n",
    "bigrams = Counter()\n",
    "trigrams = Counter()\n",
    "for line in text:\n",
    "    words_in_line = line.strip().split()\n",
    "    unigrams.update(words_in_line)  \n",
    "    bigrams.update(generate_ngrams(words_in_line, 2))  \n",
    "    trigrams.update(generate_ngrams(words_in_line, 3))  \n",
    "    words.extend(line.split(\" \"))\n",
    "\n",
    "words=[word for word in words if word!='']\n",
    "\n",
    "vocab=set(words)\n",
    "print(\"unique words in vocab = \",len(vocab))\n",
    "word_count_dict = {}\n",
    "word_count_dict = Counter(words)\n",
    "probs = {}\n",
    "m = sum(word_count_dict.values())\n",
    "for k, v in word_count_dict.items():\n",
    "    probs[k] = v / m\n",
    "\n",
    "def delete_letter(word):\n",
    "    letters = utf8.get_letters(word)\n",
    "\n",
    "    delete_l = []\n",
    "    if len(letters)<3:\n",
    "        return word\n",
    "\n",
    "    for i in range(1,len(letters)):\n",
    "        new_word=''.join(letters[:i])\n",
    "        if len(letters)>i+1:\n",
    "            new_word+=''.join(letters[i+1:])\n",
    "        delete_l.append(new_word)\n",
    "\n",
    "    return delete_l\n",
    "\n",
    "def insert_letter(word):\n",
    "    letters = utf8.get_letters(word)\n",
    "    insert_l=[]\n",
    "\n",
    "    for l in ['க', 'ச', 'ட', 'த', 'ப', 'ற',\n",
    "                'ம', 'ய', 'ர', 'வ', 'ஜ', 'ஸ']:\n",
    "        x=copy.deepcopy(letters)\n",
    "        x.append(l+'்')\n",
    "        insert_l.append(''.join(x))\n",
    "\n",
    "    return insert_l\n",
    "\n",
    "def transpose_letter(word):\n",
    "    letters = utf8.get_letters(word)\n",
    "    transpose_l=[]\n",
    "    for i in range(1,len(letters)-1):\n",
    "        x=copy.deepcopy(letters)\n",
    "        x[i],x[i+1]=x[i+1],x[i]\n",
    "        transpose_l.append(''.join(x))\n",
    "\n",
    "    return transpose_l\n",
    "\n",
    "def substitute_letter(word):\n",
    "    letters = utf8.get_letters(word)\n",
    "    target_chars = [['ர','ற'],['ல','ள','ழ'],['ன','ண','ந'],['ங','ஞ']]\n",
    "    substitute_l=[]\n",
    "    for i in range(0,len(letters)):\n",
    "\n",
    "        for l in target_chars:\n",
    "            if letters[i][0] in l:\n",
    "                for char in [char for char in l if char != letters[i][0]]:\n",
    "                    x=copy.deepcopy(letters)\n",
    "                    if len(x[i])==2:\n",
    "                        x[i]=char+x[i][1]\n",
    "                    else:\n",
    "                        x[i]=char\n",
    "                    substitute_l.append(''.join(x))\n",
    "\n",
    "    target_chars = [[ 'ி', 'ீ'],['','ா','ை'],['ு', 'ூ'],['ெ', 'ே'],['ொ', 'ோ']]\n",
    "    for i in range(0,len(letters)):\n",
    "\n",
    "        for l in target_chars:\n",
    "            if len(letters[i])==1 and letters[i] in ['க', 'ச', 'ட', 'த', 'ப', 'ற', 'ஞ', 'ங', 'ண', 'ந',\n",
    "                   'ம','ன', 'ய', 'ர', 'ல', 'வ', 'ழ', 'ள', 'ஜ', 'ஷ', 'ஸ', 'ஹ']:\n",
    "                for char in ['ா','ை']:\n",
    "                    x=copy.deepcopy(letters)\n",
    "                    x[i]+=char\n",
    "                    substitute_l.append(''.join(x))\n",
    "            elif len(letters[i])==2 and letters[i][1] in l:\n",
    "                for char in [char for char in l if char != letters[i][1]]:\n",
    "                    x=copy.deepcopy(letters)\n",
    "                    x[i]=x[i][0]+char\n",
    "                    substitute_l.append(''.join(x))\n",
    "\n",
    "    return substitute_l\n",
    "\n",
    "def edit_one_letter(word):\n",
    "    edit_one_set = set()\n",
    "    edit_one_set.update(delete_letter(word))\n",
    "    edit_one_set.update(transpose_letter(word))\n",
    "    edit_one_set.update(substitute_letter(word))\n",
    "    edit_one_set.update(insert_letter(word))\n",
    "\n",
    "    return edit_one_set\n",
    "\n",
    "words_outof_context=0\n",
    "\n",
    "# Compute Context Probability (Using Bigrams and Trigrams)\n",
    "def ngram_probability(word, prev_word, prev_prev_word):\n",
    "    \n",
    "    unigram_prob = unigrams.get(word, 0) / sum(unigrams.values())\n",
    "    bigram_prob = bigrams.get((prev_word, word), 0) / unigrams.get(prev_word, 1)\n",
    "    trigram_prob = trigrams.get((prev_prev_word, prev_word, word), 0) / bigrams.get((prev_prev_word, prev_word), 1)\n",
    "\n",
    "    # Weighted probability combination \n",
    "    return 0.5 * trigram_prob + 0.4 * bigram_prob + 0.1 * unigram_prob\n",
    "\n",
    "def normalize_scores(scores):\n",
    "    min_score = min(scores.values())\n",
    "    max_score = max(scores.values())\n",
    "\n",
    "    # Prevent division by zero if min == max\n",
    "    if min_score == max_score:\n",
    "        return {key: 1.0 for key in scores}  # Return all normalized to 1 if all scores are the same\n",
    "\n",
    "    return {key: (value - min_score) / (max_score - min_score) for key, value in scores.items()}\n",
    "\n",
    "def log_transform_scores(scores):\n",
    "    \"\"\"Apply log transformation to ngram scores.\"\"\"\n",
    "    return {key: math.log(value + 1e-8) for key, value in scores.items()}\n",
    "\n",
    "fasttext_proj = torch.nn.Linear(300, 768).to(device)\n",
    "attention_layer = torch.nn.MultiheadAttention(embed_dim=768, num_heads=4, batch_first=True).to(device)\n",
    "projection_layer = torch.nn.Linear(1536, 768).to(device)\n",
    "contrastive_loss_fn = torch.nn.CosineEmbeddingLoss()\n",
    "\n",
    "def get_mt5_ranking(sentence, word, suggestions):\n",
    "    \"\"\"Generate suggestions using mBERT if the word is OOV and rank all suggestions using MLM-style ranking.\"\"\"\n",
    "    mt5suggestions = copy.deepcopy(suggestions)\n",
    "    ln = len(mt5suggestions)\n",
    "    qa_scores = {}\n",
    "    \n",
    "    # Generate new suggestions if the list is empty\n",
    "    cleaned_suggestions = set(mt5suggestions)\n",
    "    if len(mt5suggestions)<=1:\n",
    "        fasttext_suggestions = get_fasttext_suggestions(word)\n",
    "        mt5suggestions.extend(fasttext_suggestions.keys())\n",
    "        \n",
    "    \n",
    "    for suggestion in mt5suggestions:\n",
    "        suggestion = re.sub(r\"[a-zA-Z0-9]\", \"\", suggestion)\n",
    "        suggestion = re.sub(r\"[^\\u0B80-\\u0BFF\\s]\", \"\", suggestion).strip()\n",
    "        if suggestion:  # Ensure it's not empty\n",
    "            cleaned_suggestions.add(suggestion)\n",
    "\n",
    "    fasttext_word_embedding = fasttext_proj(torch.tensor(ft.get_word_vector(word)).unsqueeze(0).to(device))\n",
    "\n",
    "\n",
    "    fasttext_sentence_embedding = fasttext_proj(torch.tensor(ft.get_sentence_vector(sentence)).unsqueeze(0).to(device))\n",
    "    \n",
    "    for suggestion in cleaned_suggestions:\n",
    "        input_text = f\"Is '{suggestion}' the correct spelling for '{word}' in this sentence: {sentence}?\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        fasttext_suggestion_embedding = fasttext_proj(torch.tensor(ft.get_word_vector(suggestion)).unsqueeze(0).to(device))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "           \n",
    "            inputs_embeds = qa_model.shared(inputs.input_ids)  \n",
    "            \n",
    "            fasttext_combined = torch.cat([fasttext_sentence_embedding, fasttext_suggestion_embedding], dim=0).unsqueeze(0)  \n",
    "            fasttext_combined = fasttext_combined.repeat(1, inputs_embeds.shape[1], 1)\n",
    "\n",
    "            # Apply Multihead Attention\n",
    "            attn_output, _ = attention_layer(inputs_embeds, fasttext_combined, fasttext_combined)\n",
    "\n",
    "            # Concatenate and pass through model\n",
    "            combined_embeds = torch.cat([inputs_embeds, attn_output], dim=-1)\n",
    "            combined_embeds = projection_layer(combined_embeds)\n",
    "            outputs = qa_model(inputs_embeds=combined_embeds, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss.item()    \n",
    "            \n",
    "        target = torch.tensor([1.0]).to(device)  # Positive pair\n",
    "        contrastive_loss = contrastive_loss_fn(fasttext_word_embedding, fasttext_suggestion_embedding, target)\n",
    "        loss = loss + contrastive_loss.item()\n",
    "        qa_scores[suggestion] = 1 / (1 + loss)  # Convert loss to a positive score\n",
    "\n",
    "        if(ln==1 and len(cleaned_suggestions)>3):\n",
    "            fasttext_distance = fasttext_suggestions.get(suggestion)  \n",
    "            if fasttext_distance is not None:\n",
    "                distance_penalty = 1 / (1 + fasttext_distance)  # Higher penalty for larger distance\n",
    "                qa_scores[suggestion] *= distance_penalty  # Reduce score for distant words\n",
    "        \n",
    "        \n",
    "    # Normalize scores\n",
    "    total_score = sum(qa_scores.values())\n",
    "    if total_score > 0:\n",
    "        qa_scores = {w: score / total_score for w, score in qa_scores.items()}\n",
    "    \n",
    "    return qa_scores\n",
    "\n",
    "\n",
    "def get_corrections(word, sentence, probs, vocab, prev_word, prev_prev_word, n=3):\n",
    "\n",
    "    ngram = {}\n",
    "    bert = {}\n",
    "    final_scores = {}\n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    if word in vocab and ngram_probability(word, prev_word, prev_prev_word) > 0.00001:\n",
    "        return 'Nil', {word}, []\n",
    "\n",
    "    one_error_set = edit_one_letter(word)\n",
    "    suggestions = one_error_set.intersection(vocab)\n",
    "    one_error = list(one_error_set)\n",
    "    letters = utf8.get_letters(word)\n",
    "    two_error_set = set()\n",
    "    for i in one_error:\n",
    "        two_error_set = two_error_set.union(edit_one_letter(i))\n",
    "        \n",
    "        suggestions = suggestions.union(two_error_set.intersection(vocab))\n",
    "    \n",
    "    suggestions = list(suggestions)\n",
    "    \n",
    "    suggestions.append(word)\n",
    "\n",
    "    n_best_list_tuple = sorted(\n",
    "        [(word, probs[word]) if word in probs else (word, 0) for word in suggestions]\n",
    ",\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    n_best_list = [word for word, _ in n_best_list_tuple]\n",
    "    n_best_list = list(dict.fromkeys(n_best_list))\n",
    "    l = n_best_list[6:10]\n",
    "    n_best_list = n_best_list[:6]\n",
    "    sentence_text = \" \".join(sentence)\n",
    "    ln=len(n_best_list)        \n",
    "    \n",
    "    for wrd in n_best_list:\n",
    "        ngram[wrd] = ngram_probability(wrd, prev_word, prev_prev_word)\n",
    "     \n",
    "    ngram = log_transform_scores(ngram)\n",
    "    ngram = normalize_scores(ngram)\n",
    "    \n",
    "    bert = get_mt5_ranking(sentence_text, word, n_best_list)\n",
    "    sorted_bert = sorted(bert.items(), key=lambda x: x[1], reverse=True)\n",
    "    for i, (suggestion, score) in enumerate(sorted_bert):\n",
    "      if suggestion in ngram:\n",
    "        if suggestion!=word and ngram[suggestion]>0.1:\n",
    "            final_scores[suggestion] = 0.7 * score + 0.3 * ngram[suggestion]\n",
    "        else:\n",
    "            final_scores[suggestion] = ngram[suggestion]\n",
    "      else:\n",
    "        final_scores[suggestion] = score\n",
    "          \n",
    "    n_best_list = [suggestion for suggestion, _ in sorted_bert]\n",
    "    n_best_list =sorted(n_best_list, key=lambda w: final_scores[w], reverse=True)\n",
    "    n_best_list.extend(l)\n",
    "    n_best_list = n_best_list[:3]\n",
    "    n_best=set(n_best_list)\n",
    "    return n_best_list, n_best, final_scores\n",
    "\n",
    "corrected=0\n",
    "missed=0\n",
    "file_no=1\n",
    "with open('/kaggle/input/clean-test-data/error_details_5.txt', 'r',encoding='utf-8') as file:\n",
    "    values = file.readlines()\n",
    "\n",
    "no_error_list = [int(value.strip()) for value in values[0].split(',')]\n",
    "no_error_index = set(no_error_list)\n",
    "\n",
    "single_error_list = [int(value.strip()) for value in values[1].split(',')]\n",
    "single_error_index = set(single_error_list)\n",
    "\n",
    "double_error_list = [int(value.strip()) for value in values[2].split(',')]\n",
    "double_error_index = set(double_error_list)\n",
    "\n",
    "with open('/kaggle/input/clean-test-data/error_file_5.txt', 'r', encoding='utf-8') as file:\n",
    "    test_data = file.readlines()\n",
    "\n",
    "test_data = remove_parenthesis_text(test_data)\n",
    "for i in range(len(test_data)):\n",
    "    test_data[i]=test_data[i].split(\" \")\n",
    "test_data = [[word for word in line if word] for line in test_data]\n",
    "word_cnt=0\n",
    "\n",
    "with open(\"/kaggle/input/clean-test-data/clean_test_data.txt\", 'r',encoding='utf-8') as file:\n",
    "    test_data_correct=file.readlines()\n",
    "    test_data_correct = remove_parenthesis_text(test_data_correct)\n",
    "    test_data_correct=test_data_correct[1200:1500]\n",
    "    test_data_correct = [clean_line(line) for line in test_data_correct]\n",
    "\n",
    "for i in range(len(test_data_correct)):\n",
    "    test_data_correct[i]=test_data_correct[i].split(\" \")\n",
    "\n",
    "test_data_correct = [[word for word in line if word] for line in test_data_correct]\n",
    "\n",
    "not_in_vocab=0\n",
    "correctly_predicted_no_error=0\n",
    "correctly_predicted_single_error=0\n",
    "correctly_predicted_top_suggestion=0\n",
    "correctly_predicted_double_error=0\n",
    "correctly_given_firstoption_no_error=0\n",
    "\n",
    "punctuations = {'.',',','\"',\"'\"}\n",
    "for i in range(len(test_data)):\n",
    "    prev_word=\"<s>\"\n",
    "    prev_prev_word=\"<s>\"\n",
    "    crct_sentence=test_data[i]\n",
    "    for j in range(len(test_data[i])):\n",
    "        if test_data[i][j] in punctuations:\n",
    "            continue\n",
    "\n",
    "        chk_list,chk, scores = get_corrections(test_data[i][j], crct_sentence, probs,vocab,prev_word,prev_prev_word)\n",
    "        if len(chk)>0 and (j+1)!=len(test_data[i]) and test_data[i][j+1][0] in [\"க\", \"ச\", \"ட\", \"த\",\"ப\",\"ற\"]:\n",
    "            if(chk_list=='Nil'):\n",
    "                chk_list = list(chk)\n",
    "            expected_sandhi=test_data[i][j+1][0]+'்'\n",
    "            length=len(chk)\n",
    "            for k in range(length):\n",
    "                if chk_list[k][-1]!='்':\n",
    "                    chk_list.append(chk_list[k]+expected_sandhi)\n",
    "                    chk.add(chk_list[k]+expected_sandhi)\n",
    "        if (word_cnt in no_error_index):\n",
    "            if chk_list==\"Nil\" or test_data_correct[i][j] in chk:\n",
    "                correctly_predicted_no_error+=1\n",
    "            if chk_list==\"Nil\" or len(chk_list)>0 and chk_list[0]==test_data_correct[i][j]:\n",
    "                correctly_given_firstoption_no_error+=1\n",
    "\n",
    "        elif (word_cnt in single_error_index):\n",
    "\n",
    "                if test_data_correct[i][j] in chk:\n",
    "                    correctly_predicted_single_error+=1\n",
    "                if chk_list[0]==test_data_correct[i][j]:\n",
    "                    correctly_predicted_top_suggestion+=1\n",
    "\n",
    "        elif (word_cnt in double_error_index):\n",
    "\n",
    "                if test_data_correct[i][j] in chk:\n",
    "                    correctly_predicted_double_error+=1\n",
    "                if chk_list[0]==test_data_correct[i][j]:\n",
    "                    correctly_predicted_top_suggestion+=1\n",
    "        word_cnt+=1\n",
    "\n",
    "        if(test_data_correct[i][j] not in vocab):\n",
    "            not_in_vocab+=1\n",
    "\n",
    "        if(word_cnt%1000 == 0):\n",
    "            print(word_cnt)\n",
    "\n",
    "        prev_prev_word=prev_word\n",
    "        if(chk_list==\"Nil\" or chk_list==[]):\n",
    "            prev_word=test_data[i][j]\n",
    "        else:\n",
    "            prev_word=chk_list[0]\n",
    "            \n",
    "print(\"Correct No error prediction: \", str(correctly_predicted_no_error))\n",
    "print(\"Correct Single error prediction: \", str(correctly_predicted_single_error))\n",
    "print(\"Correct Double error prediction: \", str(correctly_predicted_double_error))\n",
    "print(\"Total no error: \", str(len(no_error_list)))\n",
    "print(\"Total single error: \", str(len(single_error_list)))\n",
    "print(\"Total double error: \", str(len(double_error_list)))\n",
    "print(\"Total Accuracy: \", str((correctly_predicted_no_error+correctly_predicted_single_error+correctly_predicted_double_error)/(len(no_error_list)+len(single_error_list)+len(double_error_list))))\n",
    "print(\"Accuracy among errors: \", str((correctly_predicted_single_error+correctly_predicted_double_error)/(len(single_error_list)+len(double_error_list))))\n",
    "print(\"Total top suggestion accuracy: \", str((correctly_predicted_top_suggestion+correctly_given_firstoption_no_error)/(len(no_error_list)+len(single_error_list)+len(double_error_list))))\n",
    "print(\"Top suggestion accuracy among errors: \", str(correctly_predicted_top_suggestion/(len(single_error_list)+len(double_error_list))))\n",
    "\n",
    "finish = time.perf_counter()\n",
    "\n",
    "print(\"Time =\",round(finish-start,2),\"sec\")\n",
    "print(\"Not in Vocab =\",not_in_vocab, not_in_vocab*100/(len(single_error_list)+len(double_error_list)+len(no_error_list)),\"%\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
