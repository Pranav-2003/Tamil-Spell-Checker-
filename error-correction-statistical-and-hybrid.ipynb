{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11422582,"sourceType":"datasetVersion","datasetId":6617480},{"sourceId":323515,"sourceType":"modelInstanceVersion","modelInstanceId":272545,"modelId":293523},{"sourceId":337362,"sourceType":"modelInstanceVersion","modelInstanceId":282221,"modelId":303092}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import login\nlogin(\"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\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:13:56.207569Z","iopub.execute_input":"2025-05-05T06:13:56.207833Z","iopub.status.idle":"2025-05-05T06:13:56.923791Z","shell.execute_reply.started":"2025-05-05T06:13:56.207805Z","shell.execute_reply":"2025-05-05T06:13:56.923026Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"pip install open-tamil Levenshtein","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:13:56.924537Z","iopub.execute_input":"2025-05-05T06:13:56.924796Z","iopub.status.idle":"2025-05-05T06:14:05.993917Z","shell.execute_reply.started":"2025-05-05T06:13:56.924772Z","shell.execute_reply":"2025-05-05T06:14:05.993195Z"}},"outputs":[{"name":"stdout","text":"Collecting open-tamil\n  Downloading Open-Tamil-1.1.tar.gz (2.6 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting Levenshtein\n  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein)\n  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nDownloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: open-tamil\n  Building wheel for open-tamil (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for open-tamil: filename=Open_Tamil-1.1-py3-none-any.whl size=2388368 sha256=c9f02ba31124bd39baa94502da82e1742afdb17215d016cfe706f6d9539ec385\n  Stored in directory: /root/.cache/pip/wheels/df/de/5a/d897a3edbefc5101587607d28b221c86f39482393daed8c18f\nSuccessfully built open-tamil\nInstalling collected packages: open-tamil, rapidfuzz, Levenshtein\nSuccessfully installed Levenshtein-0.27.1 open-tamil-1.1 rapidfuzz-3.13.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport re\nfrom collections import Counter\nimport copy\nfrom tamil import utf8\nimport time\n\nstart = time.perf_counter()\n\n\nwith open('/kaggle/input/clean-test-data/new_clean_data.txt', 'r', encoding='utf-8') as file:\n    text = file.readlines()\n\ndef remove_parenthesis_text(lines):\n    # Regular expression to match text within parentheses\n    # pattern = r'\\(.*?\\)'\n    pattern = r'\\(.?\\)|<\\/?doc.?>|[^ \\u0B80-\\u0BFF]'\n    \n    # Process each line and remove the parenthesis text\n    cleaned_lines = [re.sub(pattern, '', line).strip() for line in lines]\n    \n    return cleaned_lines\n\ndef clean_line(line):\n    # Use regex to substitute anything that's not a letter or space with an empty string\n    return re.sub(r'[^à®€-à¯¿\\s]', '', line)\n\ntext = remove_parenthesis_text(text)\n\ntext = [clean_line(line) for line in text]\n\ndef generate_ngrams(words, n):\n    \"\"\"Generate n-grams from a list of words.\"\"\"\n    return [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n\nwords=[]\nunigrams = Counter()\nbigrams = Counter()\ntrigrams = Counter()\nfor line in text:\n    words_in_line = line.strip().split()\n    unigrams.update(words_in_line)  # Unigrams\n    bigrams.update(generate_ngrams(words_in_line, 2))  # Bigrams\n    trigrams.update(generate_ngrams(words_in_line, 3))  # Trigrams\n    words.extend(line.split(\" \"))\n\n# print(\"Top 5 Unigrams:\", unigram_counts.most_common(5))\n# print(\"Top 5 Bigrams:\", bigram_counts.most_common(5))\n# print(\"Top 5 Trigrams:\", trigram_counts.most_common(5))\n\nwords=[word for word in words if word!='']\n\nvocab=set(words)\nprint(\"unique words in vocab = \",len(vocab))\nword_count_dict = {}  \nword_count_dict = Counter(words)\nprobs = {} \nm = sum(word_count_dict.values())\nfor k, v in word_count_dict.items():\n    probs[k] = v / m\n\ndef delete_letter(word):\n    letters = utf8.get_letters(word)\n    \n    delete_l = []\n    if len(letters)<3:\n        return word\n    \n    for i in range(1,len(letters)):\n        # if len(letters[i])>1 and letters[i][1]=='à¯':\n        new_word=''.join(letters[:i])\n        if len(letters)>i+1:\n            new_word+=''.join(letters[i+1:])\n        delete_l.append(new_word)\n\n    return delete_l\n\ndef insert_letter(word):\n    letters = utf8.get_letters(word)\n    insert_l=[]\n    #for i in range(1,len(letters)+1):\n        \n    for l in ['à®•', 'à®š', 'à®Ÿ', 'à®¤', 'à®ª', 'à®±',  \n                'à®®', 'à®¯', 'à®°', 'à®µ', 'à®œ', 'à®¸']:\n        x=copy.deepcopy(letters)\n        #x.insert(i,l+'à¯')\n        x.append(l+'à¯')\n        insert_l.append(''.join(x))\n    \n    return insert_l\n\ndef transpose_letter(word):\n    letters = utf8.get_letters(word)\n    transpose_l=[]\n    for i in range(1,len(letters)-1):\n        x=copy.deepcopy(letters)\n        x[i],x[i+1]=x[i+1],x[i]\n        transpose_l.append(''.join(x))\n    \n    return transpose_l    \n\ndef substitute_letter(word):\n    letters = utf8.get_letters(word)\n    target_chars = [['à®°','à®±'],['à®²','à®³','à®´'],['à®©','à®£','à®¨'],['à®™','à®'], ['à®©','à®³'], ['à®°','à®•à®³']]\n    substitute_l=[]\n    for i in range(0,len(letters)):\n        \n        for l in target_chars:\n            if letters[i][0] in l:\n                for char in [char for char in l if char != letters[i][0]]:\n                    x=copy.deepcopy(letters)\n                    if len(x[i])==2:\n                        x[i]=char+x[i][1]\n                    else:\n                        x[i]=char\n                    substitute_l.append(''.join(x))\n    \n    target_chars = [[ 'à®¿', 'à¯€'],['','à®¾','à¯ˆ'],['à¯', 'à¯‚'],['à¯†', 'à¯‡'],['à¯Š', 'à¯‹']]\n    for i in range(0,len(letters)):\n        \n        for l in target_chars:\n            if len(letters[i])==1 and letters[i] in ['à®•', 'à®š', 'à®Ÿ', 'à®¤', 'à®ª', 'à®±', 'à®', 'à®™', 'à®£', 'à®¨', \n                   'à®®','à®©', 'à®¯', 'à®°', 'à®²', 'à®µ', 'à®´', 'à®³', 'à®œ', 'à®·', 'à®¸', 'à®¹']:\n                for char in ['à®¾','à¯ˆ']:\n                    x=copy.deepcopy(letters)\n                    x[i]+=char\n                    substitute_l.append(''.join(x))\n            elif len(letters[i])==2 and letters[i][1] in l:\n                for char in [char for char in l if char != letters[i][1]]:\n                    x=copy.deepcopy(letters)\n                    x[i]=x[i][0]+char\n                    substitute_l.append(''.join(x))\n    \n    return substitute_l\n\ndef edit_one_letter(word):\n    edit_one_set = set()\n    \n    ### START CODE HERE ###\n    edit_one_set.update(delete_letter(word))\n    edit_one_set.update(transpose_letter(word))\n    edit_one_set.update(substitute_letter(word))\n    edit_one_set.update(insert_letter(word))\n    ### END CODE HERE ###\n\n    return edit_one_set\n\nwords_outof_context=0\n\n# Compute Context Probability (Using Bigrams and Trigrams)\ndef ngram_probability(word, prev_word, prev_prev_word):\n    \"\"\"Compute probability using unigrams, bigrams, and trigrams.\"\"\"\n    unigram_prob = unigrams.get(word, 0) / sum(unigrams.values())\n    bigram_prob = bigrams.get((prev_word, word), 0) / unigrams.get(prev_word, 1)\n    trigram_prob = trigrams.get((prev_prev_word, prev_word, word), 0) / bigrams.get((prev_prev_word, prev_word), 1)\n\n    # Weighted probability combination (adjustable weights)\n    return 0.5 * trigram_prob + 0.4 * bigram_prob + 0.1 * unigram_prob\n \n\ndef get_corrections_stat(word, probs, vocab,  prev_word, prev_prev_word):\n    \n    \n    suggestions = []\n    n_best = []\n    ### START CODE HERE ###\n    if word in vocab and ngram_probability(word,prev_word,prev_prev_word)>0.00001:\n        \n        return 'Nil',{word}\n    one_error_set=edit_one_letter(word)\n    suggestions=one_error_set.intersection(vocab)\n    one_error=list(one_error_set)\n    letters=utf8.get_letters(word)\n    # if len(letters)>3:\n    for i in one_error:\n        suggestions=suggestions.union(edit_one_letter(i).intersection(vocab))\n    #suggestions = list( edit_one_letter(word).intersection(vocab))\n    \n    ### END CODE HERE ###\n    suggestions=list(suggestions)\n    suggestions.append(word)\n    n_best_list_tuple = sorted(\n    [(word, probs[word]) if word in probs else (word, 0) for word in suggestions]\n,\n    key=lambda x: x[1],\n    reverse=True\n)\n    n_best_list=[word for (word,probs) in n_best_list_tuple]\n    n_best_list = list(dict.fromkeys(n_best_list))[:10]\n    #print(suggestions)\n    #print(\"entered word = \", word, \"\\nsuggestions = \", n_best)\n\n    # return n_best\n    #n_best_list=[word for word in suggestions]\n    n_best_list = sorted(n_best_list, key=lambda w: ngram_probability(w, prev_word, prev_prev_word), reverse=True) \n    n_best_list = n_best_list[:10]\n    n_best=set(n_best_list)\n    return n_best_list, n_best \n        \n#get_corrections(\"Hello\",probs,vocab,3)\n\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:14:12.454182Z","iopub.execute_input":"2025-05-05T06:14:12.454450Z","iopub.status.idle":"2025-05-05T06:14:56.249007Z","shell.execute_reply.started":"2025-05-05T06:14:12.454425Z","shell.execute_reply":"2025-05-05T06:14:56.248416Z"}},"outputs":[{"name":"stdout","text":"unique words in vocab =  1156566\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"corrected=0\nmissed=0\nfile_no=1\nwith open('/kaggle/input/clean-test-data/error_details_1.txt', 'r',encoding='utf-8') as file:\n    values = file.readlines()\n\n# values_list = [int(value.strip()) for value in values.split(',')]\n# index = set(values_list)\n# print(len(index))\n\nno_error_list = [int(value.strip()) for value in values[0].split(',')]\nno_error_index = set(no_error_list)\n\nsingle_error_list = [int(value.strip()) for value in values[1].split(',')]\nsingle_error_index = set(single_error_list)\n\ndouble_error_list = [int(value.strip()) for value in values[2].split(',')]\ndouble_error_index = set(double_error_list)\n\nwith open('/kaggle/input/clean-test-data/error_file_1.txt', 'r', encoding='utf-8') as file:\n    test_data = file.readlines()\n    \n\ntest_data = remove_parenthesis_text(test_data)\ntest_data = [clean_line(line) for line in test_data]\nfor i in range(len(test_data)):\n    test_data[i]=test_data[i].split(\" \")\n\n# print(test_data)\ntest_data = [[word for word in line if word] for line in test_data]\n\nword_cnt=0\n\n\n\nwith open(\"/kaggle/input/clean-test-data/clean_test_data.txt\", 'r',encoding='utf-8') as file:\n    test_data_correct=file.readlines()\n    test_data_correct = remove_parenthesis_text(test_data_correct)\n    test_data_correct=test_data_correct[:300]\n\n    test_data_correct = [clean_line(line) for line in test_data_correct]\nx=0\nfor i in range(len(test_data_correct)):\n    test_data_correct[i]=test_data_correct[i].split(\" \")\n    # print(len(test_data[i]),len(test_data_correct[i]))\n    if (len(test_data[i])==len(test_data_correct[i])):\n        x+=1\n# print(x)\n\ntest_data_correct = [[word for word in line if word] for line in test_data_correct]\n\n\n# print(test_data)\n    \nnot_in_vocab=0    \n\n\n\n\ncorrectly_predicted_no_error=0\ncorrectly_predicted_single_error=0\ncorrectly_predicted_top_suggestion=0\ncorrectly_predicted_double_error=0\ncorrectly_given_firstoption_no_error=0\n\npunctuations = {'.',',','\"',\"'\"}\n#test_data=test_data[:10]\n#count_for_printing=0\n\nfor i in range(len(test_data)):\n    prev_word=\"<s>\"\n    prev_prev_word=\"<s>\"\n    for j in range(len(test_data[i])):\n        if test_data[i][j] in punctuations:\n            continue\n        \n        chk_list,chk = get_corrections_stat(test_data[i][j],probs,vocab,prev_word,prev_prev_word)\n            \n        # if len(chk)>0 and (j+1)!=len(test_data[i]) and test_data[i][j+1][0] in [\"à®•\", \"à®š\", \"à®Ÿ\", \"à®¤\",\"à®ª\",\"à®±\"]:\n        #     chk_list = list(chk)\n        #     sandhi_words = []  # Store sandhi-modified words separately\n\n        #     expected_sandhi = test_data[i][j + 1][0] + 'à¯'\n    \n        #     for word in chk_list:\n        #         if word[-1] != 'à¯':  # Apply sandhi rule only if not already ending in 'à¯'\n        #             sandhi_word = word + expected_sandhi\n        #             sandhi_words.append(sandhi_word)  # Add sandhi word to separate list\n        #             chk.add(sandhi_word)\n\n        #     # Merge: Sandhi words first, then original words\n        #     chk_list = sandhi_words + chk_list\n        if len(chk)>0 and (j+1)!=len(test_data[i]) and test_data[i][j+1][0] in [\"à®•\", \"à®š\", \"à®Ÿ\", \"à®¤\",\"à®ª\",\"à®±\"]:\n            if(chk_list=='Nil'):\n                chk_list = list(chk)\n                \n            expected_sandhi=test_data[i][j+1][0]+'à¯'\n            length=len(chk)\n            for k in range(length):\n                if chk_list[k][-1]!='à¯':\n                    chk_list.append(chk_list[k]+expected_sandhi)\n                    chk.add(chk_list[k]+expected_sandhi)\n            #chk=set(chk_list)\n        # if(test_data[i][j]=='à®¨à¯‹à®¯à¯à®•à®³à¯ˆ'):\n        #     print(chk_list, chk)\n        #print(test_data[i][j],test_data_correct[i][j],chk_list)\n        #print(chk_list,chk)\n        if (word_cnt in no_error_index):\n            if chk_list==\"Nil\" or test_data_correct[i][j] in chk:\n                correctly_predicted_no_error+=1\n            # else:\n            #     print(1)\n            if chk_list==\"Nil\" or (len(chk_list)>0 and chk_list[0]==test_data_correct[i][j]):\n                correctly_given_firstoption_no_error+=1\n\n        elif (word_cnt in single_error_index):\n            \n                if test_data_correct[i][j] in chk:\n                    correctly_predicted_single_error+=1\n                # else:\n                #     print(test_data[i][j], chk)\n                if chk_list[0]==test_data_correct[i][j]:\n                    correctly_predicted_top_suggestion+=1\n                # if chk_list==\"Nil\" and ngram_probability(test_data[i][j],prev_word,prev_prev_word)<0.000001:\n                #     words_outof_context+=1\n                # if chk_list==\"Nil\" and count_for_printing<50:\n                #     print(ngram_probability(test_data[i][j],prev_word,prev_prev_word))\n                #     count_for_printing+=1\n            \n        elif (word_cnt in double_error_index):\n            \n                if test_data_correct[i][j] in chk:\n                    correctly_predicted_double_error+=1\n                # else:\n                #     print(3)\n                if chk_list[0]==test_data_correct[i][j]:\n                    correctly_predicted_top_suggestion+=1\n                # if chk_list==\"Nil\" and ngram_probability(test_data[i][j],prev_word,prev_prev_word)<0.000001:\n                #     words_outof_context+=1\n                # if chk_list==\"Nil\" and count_for_printing<50:\n                #     print(ngram_probability(test_data[i][j],prev_word,prev_prev_word))\n                #     count_for_printing+=1\n        word_cnt+=1\n\n        if(test_data_correct[i][j] not in vocab):\n            not_in_vocab+=1\n\n        if(word_cnt%1000 == 0):\n            print(word_cnt)\n        \n        prev_prev_word=prev_word\n        if(chk_list==\"Nil\" or chk_list==[]):\n            prev_word=test_data[i][j]\n        else:\n            prev_word=chk_list[0]\n\n      \n        \nprint(\"Total number of words: \", str(len(single_error_list)+len(double_error_list)+len(no_error_list)))\nprint(\"Total number of errors in the file: \", str(len(single_error_list)+len(double_error_list)))\nprint(\"Total Correction Accuracy: \", str((correctly_predicted_no_error+correctly_predicted_single_error+correctly_predicted_double_error)/(len(no_error_list)+len(single_error_list)+len(double_error_list))))\nprint(\"Top-1 suggestion accuracy: \", str((correctly_predicted_top_suggestion+correctly_given_firstoption_no_error)/(len(no_error_list)+len(single_error_list)+len(double_error_list))))\n\nfinish = time.perf_counter()\n\nprint(\"Time =\",round(finish-start,2),\"sec\")\n# print(\"Words out of correction = \",words_outof_context)\nprint(\"Not in Vocab =\",not_in_vocab, not_in_vocab*100/(len(single_error_list)+len(double_error_list)+len(no_error_list)),\"%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T02:48:29.453095Z","iopub.execute_input":"2025-04-16T02:48:29.453387Z","iopub.status.idle":"2025-04-16T02:51:11.763343Z","shell.execute_reply.started":"2025-04-16T02:48:29.453366Z","shell.execute_reply":"2025-04-16T02:51:11.762646Z"}},"outputs":[{"name":"stdout","text":"1000\n2000\nTotal number of words:  2638\nTotal number of errors in the file:  1086\nTotal Correction Accuracy:  0.9768764215314633\nTop-1 suggestion accuracy:  0.6880212282031842\nTime = 249.68 sec\nNot in Vocab = 116 4.397270659590599 %\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import MT5Tokenizer,MT5ForConditionalGeneration, logging, AutoTokenizer, AutoModelForTokenClassification\nimport torch\nfrom scipy.spatial.distance import cosine\nfrom huggingface_hub import hf_hub_download\nimport os, math, string\nimport re, Levenshtein\nfrom collections import Counter\nimport copy\nfrom tamil import utf8\nimport time\n\n\ndef predict_tags(tamil_sentence):\n    model.eval()\n    label_list = [\"O\", \"B-ERR\"]\n    label_to_id = {label: i for i, label in enumerate(label_list)}\n    id_to_label = {i: label for label, i in label_to_id.items()}\n    tokens = tamil_sentence.strip().split()\n    encoding = tokenizer(words, is_split_into_words=True, return_tensors=\"pt\", truncation=True)\n    with torch.no_grad():\n        outputs = model(**encoding)\n        predictions = torch.argmax(outputs.logits, dim=2)\n    \n    word_ids = encoding.word_ids()\n    pred_labels = [id_to_label[pred.item()] for pred in predictions[0]]\n    \n    final_output = {}\n    for idx, word_idx in enumerate(word_ids):\n        if word_idx is None:\n            continue\n        word = words[word_idx]\n        if word not in final_output:\n            final_output[word] = pred_labels[idx]\n    \n    print(\"\\n\\U0001F50D Inference Results:\")\n    for word, label in final_output.items():\n        status = \"\\u2705 OK\" if label == \"O\" else \"\\u274C ERROR\"\n        print(f\"{word:<20} â†’ {label:<6}  {status}\")\n\ndef get_fasttext_suggestions(word, top_n=6):\n    try:\n        suggestions = {}  \n        #suggestions = []\n        for suggestion in ft.get_nearest_neighbors(word, k=top_n):  \n            suggested_word = suggestion[1]  # Extract the word  \n            dist = Levenshtein.distance(word, suggested_word)  \n    \n            if dist <= 4:  # Only keep words within max distance\n                suggestions[suggested_word] = dist  # Store in dict\n                #suggestions.append(suggestion)\n\n        return suggestions\n    except Exception as e:\n        return []\n\nmodel_name = \"google/mt5-base\"  # You can replace this with the appropriate model for Tamil\nqa_model = MT5ForConditionalGeneration.from_pretrained(model_name)\nqa_tokenizer = MT5Tokenizer.from_pretrained(model_name)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nqa_model.to(device)\n\n\nstart = time.perf_counter()\n\n\nwith open('/kaggle/input/clean-test-data/new_clean_data.txt', 'r', encoding='utf-8') as file:\n    text = file.readlines()\n\ndef remove_parenthesis_text(lines):\n    # Regular expression to match text within parentheses\n    # pattern = r'\\(.*?\\)'\n    pattern = r'\\(.?\\)|<\\/?doc.?>|[^ \\u0B80-\\u0BFF]'\n\n    # Process each line and remove the parenthesis text\n    cleaned_lines = [re.sub(pattern, '', line).strip() for line in lines]\n\n    return cleaned_lines\n\ndef clean_line(line):\n    # Use regex to substitute anything that's not a letter or space with an empty string\n    return re.sub(r'[^à®€-à¯¿\\s]', '', line)\n\ntext = remove_parenthesis_text(text)\n\ntext = [clean_line(line) for line in text]\n\ndef generate_ngrams(words, n):\n    \"\"\"Generate n-grams from a list of words.\"\"\"\n    return [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n\nwords=[]\nunigrams = Counter()\nbigrams = Counter()\ntrigrams = Counter()\nfor line in text:\n    words_in_line = line.strip().split()\n    unigrams.update(words_in_line)  # Unigrams\n    bigrams.update(generate_ngrams(words_in_line, 2))  # Bigrams\n    trigrams.update(generate_ngrams(words_in_line, 3))  # Trigrams\n    words.extend(line.split(\" \"))\n\n# print(\"Top 5 Unigrams:\", unigram_counts.most_common(5))\n# print(\"Top 5 Bigrams:\", bigram_counts.most_common(5))\n# print(\"Top 5 Trigrams:\", trigram_counts.most_common(5))\n\nwords=[word for word in words if word!='']\n\nvocab=set(words)\nprint(\"unique words in vocab = \",len(vocab))","metadata":{"execution":{"iopub.status.busy":"2025-05-05T06:14:56.250128Z","iopub.execute_input":"2025-05-05T06:14:56.250368Z","iopub.status.idle":"2025-05-05T06:16:40.809704Z","shell.execute_reply.started":"2025-05-05T06:14:56.250334Z","shell.execute_reply":"2025-05-05T06:16:40.808873Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2025-05-05 06:15:13.312851: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746425713.508356      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746425713.561878      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"550a7e5936144595a9508a170b80539b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bca586933af54e82a54f1906f4c0e596"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab79db9b817d44968f6a99158db46a05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4bfecd1f09d4c65b2dff8aa4d435c71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2768816d4dce46679f9a281a5c709655"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd52e36d057c4bb595f9f1b7d08df066"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48b720b929434467ab61e8f5e73c7ced"}},"metadata":{}},{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'T5Tokenizer'. \nThe class this function is called from is 'MT5Tokenizer'.\nYou are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"name":"stdout","text":"unique words in vocab =  1156566\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"logging.set_verbosity_error()\nimport fasttext.util\n\n\n\nos.environ['FASTTEXT_VERBOSE'] = '0'\nfasttext.util.download_model('ta', if_exists='ignore')  # English\nft = fasttext.load_model('cc.ta.300.bin')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-05T04:57:21.382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Load the saved model and tokenizer\nmodel_path = \"/kaggle/input/tamil_spell_detection/pytorch/default/1\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForTokenClassification.from_pretrained(model_path)\nmodel.eval()\n\n# Label mapping\nid_to_label = {0: \"O\", 1: \"B-ERR\"}\n\ndef predict(sentence):\n    words = sentence.strip().split()\n    encoding = tokenizer(words, is_split_into_words=True, return_tensors=\"pt\", truncation=True)\n\n    with torch.no_grad():\n        outputs = model(**encoding)\n        predictions = torch.argmax(outputs.logits, dim=2)\n\n    word_ids = encoding.word_ids()\n    pred_labels = [id_to_label[pred.item()] for pred in predictions[0]]\n\n    result = []\n    for idx, word_idx in enumerate(word_ids):\n        if word_idx is None:\n            continue\n        word = words[word_idx]\n        if word_idx >= len(result):  # avoid duplicates\n            result.append((word, pred_labels[idx]))\n    return result\n\n\n\ntamil_sentence = \"à®…à®µà®³à¯ à®ªà®³à¯à®³à®¿à®•à¯à®•à¯à®šà¯ à®šà¯†à®©à¯à®±à®¾\"\npredictions = predict(tamil_sentence)\n\nprint(\"\\nğŸ” Inference Results:\")\n\nindex = 2\nword, label = predictions[index]\nprint(f\"\\nğŸ” Word at index {index}: '{word}' â†’ {label}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-05T04:57:21.382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"word_count_dict = {}\nword_count_dict = Counter(words)\nprobs = {}\nm = sum(word_count_dict.values())\nfor k, v in word_count_dict.items():\n    probs[k] = v / m\n\ndef delete_letter(word):\n    letters = utf8.get_letters(word)\n\n    delete_l = []\n    if len(letters)<3:\n        return word\n\n    for i in range(1,len(letters)):\n        new_word=''.join(letters[:i])\n        if len(letters)>i+1:\n            new_word+=''.join(letters[i+1:])\n        delete_l.append(new_word)\n\n    return delete_l\n\ndef insert_letter(word):\n    letters = utf8.get_letters(word)\n    insert_l=[]\n\n    for l in ['à®•', 'à®š', 'à®Ÿ', 'à®¤', 'à®ª', 'à®±',\n                'à®®', 'à®¯', 'à®°', 'à®µ', 'à®œ', 'à®¸']:\n        x=copy.deepcopy(letters)\n        x.append(l+'à¯')\n        insert_l.append(''.join(x))\n\n    return insert_l\n\ndef transpose_letter(word):\n    letters = utf8.get_letters(word)\n    transpose_l=[]\n    for i in range(1,len(letters)-1):\n        x=copy.deepcopy(letters)\n        x[i],x[i+1]=x[i+1],x[i]\n        transpose_l.append(''.join(x))\n\n    return transpose_l\n\ndef substitute_letter(word):\n    letters = utf8.get_letters(word)\n    target_chars = [['à®°','à®±'],['à®²','à®³','à®´'],['à®©','à®£','à®¨'],['à®™','à®'], ['à®©','à®³'], ['à®°','à®•à®³']]\n    substitute_l=[]\n    for i in range(0,len(letters)):\n\n        for l in target_chars:\n            if letters[i][0] in l:\n                for char in [char for char in l if char != letters[i][0]]:\n                    x=copy.deepcopy(letters)\n                    if len(x[i])==2:\n                        x[i]=char+x[i][1]\n                    else:\n                        x[i]=char\n                    substitute_l.append(''.join(x))\n\n    target_chars = [[ 'à®¿', 'à¯€'],['','à®¾','à¯ˆ'],['à¯', 'à¯‚'],['à¯†', 'à¯‡'],['à¯Š', 'à¯‹']]\n    for i in range(0,len(letters)):\n\n        for l in target_chars:\n            if len(letters[i])==1 and letters[i] in ['à®•', 'à®š', 'à®Ÿ', 'à®¤', 'à®ª', 'à®±', 'à®', 'à®™', 'à®£', 'à®¨',\n                   'à®®','à®©', 'à®¯', 'à®°', 'à®²', 'à®µ', 'à®´', 'à®³', 'à®œ', 'à®·', 'à®¸', 'à®¹']:\n                for char in ['à®¾','à¯ˆ']:\n                    x=copy.deepcopy(letters)\n                    x[i]+=char\n                    substitute_l.append(''.join(x))\n            elif len(letters[i])==2 and letters[i][1] in l:\n                for char in [char for char in l if char != letters[i][1]]:\n                    x=copy.deepcopy(letters)\n                    x[i]=x[i][0]+char\n                    substitute_l.append(''.join(x))\n\n    return substitute_l\n\ndef edit_one_letter(word):\n    edit_one_set = set()\n    edit_one_set.update(delete_letter(word))\n    edit_one_set.update(transpose_letter(word))\n    edit_one_set.update(substitute_letter(word))\n    edit_one_set.update(insert_letter(word))\n\n    return edit_one_set\n\nwords_outof_context=0\n\n# Compute Context Probability\ndef ngram_probability(word, prev_word, prev_prev_word):\n    \n    unigram_prob = unigrams.get(word, 0) / sum(unigrams.values())\n    bigram_prob = bigrams.get((prev_word, word), 0) / unigrams.get(prev_word, 1)\n    trigram_prob = trigrams.get((prev_prev_word, prev_word, word), 0) / bigrams.get((prev_prev_word, prev_word), 1)\n\n    return 0.5 * trigram_prob + 0.4 * bigram_prob + 0.1 * unigram_prob\n\ndef normalize_scores(scores):\n    min_score = min(scores.values())\n    max_score = max(scores.values())\n\n    if min_score == max_score:\n        return {key: 1.0 for key in scores}  # Return all normalized to 1 if all scores are the same\n\n    return {key: (value - min_score) / (max_score - min_score) for key, value in scores.items()}\n\ndef log_transform_scores(scores):\n    return {key: math.log(value + 1e-8) for key, value in scores.items()}\n\nfasttext_proj = torch.nn.Linear(300, 768).to(device)\nattention_layer = torch.nn.MultiheadAttention(embed_dim=768, num_heads=4, batch_first=True).to(device)\nprojection_layer = torch.nn.Linear(1536, 768).to(device)\ncontrastive_loss_fn = torch.nn.CosineEmbeddingLoss()\n\ndef get_mt5_ranking(sentence, word, suggestions, avg, flag):\n\n    mt5suggestions = copy.deepcopy(suggestions)\n    ln = len(mt5suggestions)\n    qa_scores = {}\n    \n    # Generate new suggestions if the list has only the input word\n    cleaned_suggestions = set()\n    fasttext_suggestions = {}\n    if avg<0.0000005 or len(mt5suggestions)<=1 or flag==1:\n        fasttext_suggestions = get_fasttext_suggestions(word)\n        mt5suggestions.extend(fasttext_suggestions.keys())\n        \n    \n    for suggestion in mt5suggestions:\n        suggestion = re.sub(r\"[a-zA-Z0-9]\", \"\", suggestion)\n        suggestion = re.sub(r\"[^\\u0B80-\\u0BFF\\s]\", \"\", suggestion).strip()\n        if suggestion and len(suggestion)>2: \n            cleaned_suggestions.add(suggestion)\n\n    fasttext_word_embedding = fasttext_proj(torch.tensor(ft.get_word_vector(word)).unsqueeze(0).to(device))\n\n\n    fasttext_sentence_embedding = fasttext_proj(torch.tensor(ft.get_sentence_vector(sentence)).unsqueeze(0).to(device))\n    \n    for suggestion in cleaned_suggestions:\n        input_text = f\"Is '{suggestion}' the correct spelling for '{word}' in this sentence: {sentence}?\"\n        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n        \n        fasttext_suggestion_embedding = fasttext_proj(torch.tensor(ft.get_word_vector(suggestion)).unsqueeze(0).to(device))\n        \n        with torch.no_grad():\n           \n            inputs_embeds = qa_model.shared(inputs.input_ids)  \n            \n            fasttext_combined = torch.cat([fasttext_sentence_embedding, fasttext_suggestion_embedding], dim=0).unsqueeze(0)  \n            fasttext_combined = fasttext_combined.repeat(1, inputs_embeds.shape[1], 1)\n\n            # Apply Multihead Attention\n            attn_output, _ = attention_layer(inputs_embeds, fasttext_combined, fasttext_combined)\n\n            # Concatenate and pass through model\n            combined_embeds = torch.cat([inputs_embeds, attn_output], dim=-1)\n            combined_embeds = projection_layer(combined_embeds)\n            outputs = qa_model(inputs_embeds=combined_embeds, labels=inputs[\"input_ids\"])\n            loss = outputs.loss.item()    \n            \n        target = torch.tensor([1.0]).to(device)  # Positive pair\n        contrastive_loss = contrastive_loss_fn(fasttext_word_embedding, fasttext_suggestion_embedding, target)\n        loss = loss + contrastive_loss.item()\n        qa_scores[suggestion] = 1 / (1 + loss)  # Convert loss to a positive score\n\n        if(len(cleaned_suggestions)>3):\n            fasttext_distance = fasttext_suggestions.get(suggestion)  \n            if fasttext_distance is not None:\n                distance_penalty = math.exp(-1 * fasttext_distance)  # Higher penalty for larger distance\n            else:\n                distance_penalty = math.exp(-1 * Levenshtein.distance(word, suggestion))\n            qa_scores[suggestion] *= distance_penalty  # Reduce score for distant words\n                \n        \n        \n    # Normalize scores\n    total_score = sum(qa_scores.values())\n    if total_score > 0:\n        qa_scores = {w: score / total_score for w, score in qa_scores.items()}\n    return qa_scores\n\n\ndef get_corrections(word, index, sentence, probs, vocab, prev_word, prev_prev_word, n=3):\n\n    ngram = {}\n    mt5 = {}\n    final_scores = {}\n    suggestions = []\n    n_best = []\n    # if word in vocab and ngram_probability(word, prev_word, prev_prev_word) > 0.00001:\n    #     return 'Nil', {word}, []\n    \n    if predict(\" \".join(sentence))[index][1] == \"O\" and ngram_probability(word, prev_word, prev_prev_word) > 0.00001:\n        return 'Nil', {word}, []\n\n    flag = word in vocab\n    one_error_set = edit_one_letter(word)\n    suggestions = one_error_set.intersection(vocab)\n    one_error = list(one_error_set)\n    letters = utf8.get_letters(word)\n    two_error_set = set()\n    for i in one_error:\n        two_error_set = two_error_set.union(edit_one_letter(i))\n        \n        suggestions = suggestions.union(two_error_set.intersection(vocab))\n    \n    suggestions = list(suggestions)\n    \n    #suggestions.append(word)\n    n_best_list_tuple = sorted(\n        [(word, probs[word]) if word in probs else (word, 0) for word in suggestions]\n,\n        key=lambda x: x[1],\n        reverse=True\n    )\n\n    n_best_list = [word for word, _ in n_best_list_tuple]\n    n_best_list = list(dict.fromkeys(n_best_list))\n    # l = n_best_list[6:10]\n    n_best_list = n_best_list[:12]\n    for wrd in n_best_list:\n        ngram[wrd] = ngram_probability(wrd, prev_word, prev_prev_word)\n\n    \n    \n    \n    avg = sum(ngram.values())/(len(ngram) if len(ngram)>0 else 1)\n        \n    if(len(ngram) > 0): \n        ngram = log_transform_scores(ngram)\n        ngram = normalize_scores(ngram)\n        \n    ln=len(n_best_list)\n    sentence_text = \" \".join(sentence)\n    \n    mt5 = get_mt5_ranking(sentence_text, word, n_best_list, avg, flag)\n    sorted_mt5 = sorted(mt5.items(), key=lambda x: x[1], reverse=True)\n    for i, (suggestion, score) in enumerate(sorted_mt5):\n        if(suggestion in ngram):\n            final_scores[suggestion] = 0.9 * score + 0.1 * ngram[suggestion]\n        else:\n            final_scores[suggestion] = score\n          \n    n_best_list = [suggestion for suggestion, _ in sorted_mt5]\n    n_best_list =sorted(n_best_list, key=lambda w: final_scores[w], reverse=True)\n    # n_best_list.extend(l)\n    n_best_list = n_best_list[:5] # Comment this line to get maximum accuracy of statistical model else this will give top 3 accuracy\n    n_best=set(n_best_list)\n    return n_best_list, n_best, final_scores\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-05T04:57:21.382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corrected=0\nmissed=0\nfile_no=1\nwith open('/kaggle/input/clean-test-data/error_details_10.txt', 'r',encoding='utf-8') as file:\n    values = file.readlines()\n\nno_error_list = [int(value.strip()) for value in values[0].split(',')]\nno_error_index = set(no_error_list)\n\nsingle_error_list = [int(value.strip()) for value in values[1].split(',')]\nsingle_error_index = set(single_error_list)\n\ndouble_error_list = [int(value.strip()) for value in values[2].split(',')]\ndouble_error_index = set(double_error_list)\n\nwith open('/kaggle/input/clean-test-data/error_file_10.txt', 'r', encoding='utf-8') as file:\n    test_data = file.readlines()\n\ntest_data = remove_parenthesis_text(test_data)\nfor i in range(len(test_data)):\n    test_data[i]=test_data[i].split(\" \")\ntest_data = [[word for word in line if word] for line in test_data]\nword_cnt=0\n\nwith open(\"/kaggle/input/clean-test-data/clean_test_data.txt\", 'r',encoding='utf-8') as file:\n    test_data_correct=file.readlines()\n    test_data_correct = remove_parenthesis_text(test_data_correct)\n    test_data_correct=test_data_correct[2700:3000]\n    test_data_correct = [clean_line(line) for line in test_data_correct]\n\nfor i in range(len(test_data_correct)):\n    test_data_correct[i]=test_data_correct[i].split(\" \")\n\ntest_data_correct = [[word for word in line if word] for line in test_data_correct]\n\nnot_in_vocab=0\ncorrectly_predicted_no_error=0\ncorrectly_predicted_single_error=0\ncorrectly_predicted_top_suggestion=0\ncorrectly_predicted_double_error=0\ncorrectly_given_firstoption_no_error=0\n\npunctuations = {'.',',','\"',\"'\"}\nfor i in range(len(test_data)):\n    prev_word=\"<s>\"\n    prev_prev_word=\"<s>\"\n    crct_sentence=test_data[i]\n    for j in range(len(test_data[i])):\n        if test_data[i][j] in punctuations:\n            continue\n\n        chk_list,chk, scores = get_corrections(test_data[i][j], crct_sentence, probs,vocab,prev_word,prev_prev_word)\n        if len(chk)>0 and (j+1)!=len(test_data[i]) and test_data[i][j+1][0] in [\"à®•\", \"à®š\", \"à®Ÿ\", \"à®¤\",\"à®ª\",\"à®±\"]:\n            if(chk_list=='Nil'):\n                chk_list = list(chk)\n            expected_sandhi=test_data[i][j+1][0]+'à¯'\n            length=len(chk)\n            for k in range(length):\n                if chk_list[k][-1]!='à¯':\n                    chk_list.append(chk_list[k]+expected_sandhi)\n                    chk.add(chk_list[k]+expected_sandhi)\n        if (word_cnt in no_error_index):\n            if chk_list==\"Nil\" or test_data_correct[i][j] in chk:\n                correctly_predicted_no_error+=1\n            if chk_list==\"Nil\" or len(chk_list)>0 and chk_list[0]==test_data_correct[i][j]:\n                correctly_given_firstoption_no_error+=1\n\n        elif (word_cnt in single_error_index):\n\n                if test_data_correct[i][j] in chk:\n                    correctly_predicted_single_error+=1\n                if chk_list[0]==test_data_correct[i][j]:\n                    correctly_predicted_top_suggestion+=1\n\n        elif (word_cnt in double_error_index):\n\n                if test_data_correct[i][j] in chk:\n                    correctly_predicted_double_error+=1\n                if chk_list[0]==test_data_correct[i][j]:\n                    correctly_predicted_top_suggestion+=1\n        word_cnt+=1\n\n        if(test_data_correct[i][j] not in vocab):\n            not_in_vocab+=1\n\n        prev_prev_word=prev_word\n        if(chk_list==\"Nil\" or chk_list==[]):\n            prev_word=test_data[i][j]\n        else:\n            prev_word=chk_list[0]\n            \nprint(\"Total number of words: \", str(len(single_error_list)+len(double_error_list)+len(no_error_list)))\nprint(\"Total number of errors in the file: \", str(len(single_error_list)+len(double_error_list)))\nprint(\"Total Correction Accuracy: \", str((correctly_predicted_no_error+correctly_predicted_single_error+correctly_predicted_double_error)/(len(no_error_list)+len(single_error_list)+len(double_error_list))))\nprint(\"Top-1 suggestion accuracy: \", str((correctly_predicted_top_suggestion+correctly_given_firstoption_no_error)/(len(no_error_list)+len(single_error_list)+len(double_error_list))))\nfinish = time.perf_counter()\n\nprint(\"Time =\",round(finish-start,2),\"sec\")\n#print(\"Not in Vocab =\",not_in_vocab, not_in_vocab*100/(len(single_error_list)+len(double_error_list)+len(no_error_list)),\"%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:25:52.806980Z","iopub.execute_input":"2025-04-16T04:25:52.807358Z","iopub.status.idle":"2025-04-16T04:25:52.814034Z","shell.execute_reply.started":"2025-04-16T04:25:52.807327Z","shell.execute_reply":"2025-04-16T04:25:52.813285Z"}},"outputs":[{"name":"stdout","text":"Total number of words: 2638\nTotal number of errors in the file: 1086\nTotal Correction Accuracy: 0.9840788476118272\nTop-1 suggestion accuracy: 0.765352539802881\nTime = 763.49 sec\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"l,_ = get_corrections_stat(\"à®ªà®²à®®à¯\", probs,vocab,\"à®…à®µà®©à¯\",\"<s>\")\nprint(\"Top 10 suggestions: \", l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:19:01.215555Z","iopub.execute_input":"2025-04-16T04:19:01.216220Z","iopub.status.idle":"2025-04-16T04:19:01.344674Z","shell.execute_reply.started":"2025-04-16T04:19:01.216194Z","shell.execute_reply":"2025-04-16T04:19:01.344069Z"}},"outputs":[{"name":"stdout","text":"Top 10 suggestions:  ['à®ªà®²', 'à®ªà®²à®°à¯', 'à®ªà®¾à®²à®®à¯', 'à®ªà®´à®®à¯', 'à®ªà®²à®®à¯', 'à®ªà®¾à®²', 'à®ªà®´', 'à®ªà®²à®¾', 'à®ªà®¾à®®à¯', 'à®ªà®®à¯à®ªà¯']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"l,_,s = get_corrections(\"à®ªà®²à®®à¯\", 1, \"à®…à®µà®©à¯ à®ªà®²à®®à¯ à®šà®¾à®ªà¯à®ªà®¿à®Ÿà¯à®Ÿà®¾à®©à¯\", probs,vocab,\"à®…à®µà®©à¯\",\"<s>\")\nprint(\"Top 5 suggestions: \", l)\nprint(\"Final score of suggestions: \", s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T02:55:38.871854Z","iopub.execute_input":"2025-04-16T02:55:38.872134Z","iopub.status.idle":"2025-04-16T02:55:55.571852Z","shell.execute_reply.started":"2025-04-16T02:55:38.872113Z","shell.execute_reply":"2025-04-16T02:55:55.571075Z"}},"outputs":[{"name":"stdout","text":"Top 5 suggestions:  ['à®ªà®²à®®à¯', 'à®ªà®²à®°à¯', 'à®ªà®´à®®à¯', 'à®ªà®¾à®²à®®à¯', 'à®ªà®©à®®à¯']\nFinal score of suggestions:  {'à®ªà®²à®®à¯': 0.2962973580146147, 'à®ªà®©à®®à¯': 0.10925408197563943, 'à®ªà®´à®®à¯': 0.12888155640436402, 'à®ªà®¾à®®à¯': 0.1033471069238743, 'à®ªà®¾à®²à®®à¯': 0.12673383532407156, 'à®ªà®²à®ªà¯': 0.08442154621578497, 'à®ªà®²à®°à¯': 0.13059137589615163, 'à®ªà®²à®¾': 0.04245991182965336, 'à®šà¯à®ªà®ªà®²à®®à¯': 0.015725390318624174, 'à®ªà¯à®œà®ªà®²à®®à¯': 0.014414130328480003, 'à®¤à¯‡à®•à®ªà®²à®®à¯': 0.014149732121506116, 'à®ªà®®à¯à®ªà¯': 0.016104019338969724, 'à®ªà®¾à®²': 0.03682138469770506}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"l,_ = get_corrections_stat(\"à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®©à¯\", probs,vocab,\"à®ªà¯à®¤à¯à®¤à®•à®®à¯\",\"à®…à®µà®³à¯\")\nprint(\"Top 10 suggestions: \", l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T02:56:07.797677Z","iopub.execute_input":"2025-04-16T02:56:07.797952Z","iopub.status.idle":"2025-04-16T02:56:07.929898Z","shell.execute_reply.started":"2025-04-16T02:56:07.797932Z","shell.execute_reply":"2025-04-16T02:56:07.929284Z"}},"outputs":[{"name":"stdout","text":"Top 10 suggestions:  ['à®ªà®Ÿà®¿à®¤à¯à®¤', 'à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®°à¯', 'à®ªà®¤à®¾à®©à¯', 'à®ªà¯ˆà®¤à¯à®¤à®¾à®©à¯', 'à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®²à¯', 'à®ªà®¤à¯à®¤à®¾à®©à¯', 'à®ªà®¤à¯à®¤à®©à¯', 'à®ªà®Ÿà®¿à®¤à¯', 'à®ªà®Ÿà®¿à®¤à®¾à®©à¯', 'à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®³à¯']\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"l,_,s = get_corrections(\"à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®©à¯\", 2, \"à®…à®µà®³à¯ à®ªà¯à®¤à¯à®¤à®•à®®à¯ à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®©à¯\", probs,vocab,\"à®ªà¯à®¤à¯à®¤à®•à®®à¯\",\"à®…à®µà®³à¯\")\nprint(\"Top 5 suggestions: \", l)\nprint(\"Final score of suggestions: \", s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T02:56:14.331899Z","iopub.execute_input":"2025-04-16T02:56:14.332702Z","iopub.status.idle":"2025-04-16T02:56:16.168436Z","shell.execute_reply.started":"2025-04-16T02:56:14.332674Z","shell.execute_reply":"2025-04-16T02:56:16.167802Z"}},"outputs":[{"name":"stdout","text":"Top 5 suggestions:  ['à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®°à¯', 'à®ªà®Ÿà®¿à®¤à®¤à®¾à®©à¯', 'à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®²à¯', 'à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®³à¯', 'à®ªà®Ÿà®¿à®¤à¯à®¤']\nFinal score of suggestions:  {'à®ªà®Ÿà®¿à®¤à®¤à®¾à®©à¯': 0.16939446608535044, 'à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®²à¯': 0.16019729506016978, 'à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®°à¯': 0.19303334584881282, 'à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®³à¯': 0.13391199367256837, 'à®‡à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®©à¯': 0.11624367357731133, 'à®ªà®Ÿà®¿à®¤à®¾à®©à¯': 0.05649037868723492, 'à®ªà¯ˆà®¤à¯à®¤à®¾à®©à¯': 0.0725480277466334, 'à®ªà®¤à¯à®¤à®¾à®©à¯': 0.05796256778357615, 'à®ªà®Ÿà®¿à®¤à¯à®¤': 0.12006896857528905, 'à®ªà®¤à¯à®¤à®©à¯': 0.022540571597990094, 'à®ªà®Ÿà®¿à®¤à¯à®¤à®µà®©à¯à®¤à®¾à®©à¯': 0.008047835810467899, 'à®ªà®¤à®¾à®©à¯': 0.03991539091910237, 'à®ªà®Ÿà®¿à®©à¯': 0.006843950659704523, 'à®ªà®Ÿà®¿à®¤à¯': 0.009285948745478324, 'à®ªà®¤à¯à®¤à®¾': 0.0062500571585225, 'à®ªà®Ÿà®¿à®¤à¯à®¤à¯à®¤à¯à®¤à®¾à®©à¯': 0.005863149407920641}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"l,_ = get_corrections_stat(\"à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®©à¯\", probs,vocab,\"à®ªà¯à®¤à¯à®¤à®•à®®à¯\",\"à®…à®µà®°à¯\")\nprint(\"Top 10 suggestions: \", l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T02:56:24.469851Z","iopub.execute_input":"2025-04-16T02:56:24.470118Z","iopub.status.idle":"2025-04-16T02:56:24.602866Z","shell.execute_reply.started":"2025-04-16T02:56:24.470096Z","shell.execute_reply":"2025-04-16T02:56:24.602240Z"}},"outputs":[{"name":"stdout","text":"Top 10 suggestions:  ['à®ªà®Ÿà®¿à®¤à¯à®¤', 'à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®°à¯', 'à®ªà®¤à®¾à®©à¯', 'à®ªà¯ˆà®¤à¯à®¤à®¾à®©à¯', 'à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®²à¯', 'à®ªà®¤à¯à®¤à®¾à®©à¯', 'à®ªà®¤à¯à®¤à®©à¯', 'à®ªà®Ÿà®¿à®¤à¯', 'à®ªà®Ÿà®¿à®¤à®¾à®©à¯', 'à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®³à¯']\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"l,_,s = (get_corrections(\"à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®©à¯\", 2, \"à®…à®µà®°à¯ à®ªà¯à®¤à¯à®¤à®•à®®à¯ à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®©à¯\", probs,vocab,\"à®ªà¯à®¤à¯à®¤à®•à®®à¯\",\"à®…à®µà®°à¯\"))\nprint(\"Top 5 suggestions: \", l)\nprint(\"Final score of suggestions: \", s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T02:56:29.304841Z","iopub.execute_input":"2025-04-16T02:56:29.305133Z","iopub.status.idle":"2025-04-16T02:56:31.160817Z","shell.execute_reply.started":"2025-04-16T02:56:29.305098Z","shell.execute_reply":"2025-04-16T02:56:31.160073Z"}},"outputs":[{"name":"stdout","text":"Top 5 suggestions:  ['à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®°à¯', 'à®ªà®Ÿà®¿à®¤à®¤à®¾à®©à¯', 'à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®²à¯', 'à®‡à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®©à¯', 'à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®³à¯']\nFinal score of suggestions:  {'à®ªà®Ÿà®¿à®¤à®¤à®¾à®©à¯': 0.1649074744233949, 'à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®³à¯': 0.13868452308000837, 'à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®°à¯': 0.1902157379339769, 'à®‡à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®©à¯': 0.14677356149319143, 'à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®²à¯': 0.14780067413780773, 'à®ªà®Ÿà®¿à®¤à®¾à®©à¯': 0.055466015499220325, 'à®ªà®¤à¯à®¤à®¾à®©à¯': 0.05602198224140092, 'à®ªà¯ˆà®¤à¯à®¤à®¾à®©à¯': 0.06260700883913087, 'à®ªà®Ÿà®¿à®¤à¯à®¤': 0.11925161162364367, 'à®ªà®¤à¯à®¤à®©à¯': 0.02088353147284416, 'à®ªà®¤à¯à®¤à®¾': 0.007299882236430782, 'à®ªà®¤à®¾à®©à¯': 0.040161794852984996, 'à®ªà®Ÿà®¿à®¤à¯à®¤à®µà®©à¯à®¤à®¾à®©à¯': 0.007725878056958298, 'à®ªà®Ÿà®¿à®¤à¯à®¤à¯à®¤à¯à®¤à®¾à®©à¯': 0.007632775277978438, 'à®ªà®Ÿà®¿à®¤à¯': 0.009426115724068733, 'à®ªà®Ÿà®¿à®©à¯': 0.006488110880139586}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"l,_ = get_corrections_stat(\"à®®à®²\", probs,vocab,\"à®ªà¯†à®°à®¿à®¯\",\"à®…à®¤à¯\")\nprint(\"Top 10 suggestions: \", l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T02:56:36.860948Z","iopub.execute_input":"2025-04-16T02:56:36.861461Z","iopub.status.idle":"2025-04-16T02:56:36.991907Z","shell.execute_reply.started":"2025-04-16T02:56:36.861426Z","shell.execute_reply":"2025-04-16T02:56:36.991288Z"}},"outputs":[{"name":"stdout","text":"Top 10 suggestions:  ['à®®à®²à¯ˆ', 'à®®à®²à¯ˆà®ªà¯', 'à®®à®²à¯ˆà®¤à¯', 'à®®à®©', 'à®®à®²à®°à¯', 'à®®à®²à¯ˆà®•à¯', 'à®®à®¾à®²à¯ˆ', 'à®®à®´à¯ˆ', 'à®®à®²à®¾à®¯à¯', 'à®²à®µà¯']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"l,_,s = (get_corrections(\"à®®à®²\", 2, \"à®…à®¤à¯ à®ªà¯†à®°à®¿à®¯ à®®à®²\", probs,vocab,\"à®ªà¯†à®°à®¿à®¯\",\"à®…à®¤à¯\"))\nprint(\"Top 5 suggestions: \", l)\nprint(\"Final score of suggestions: \", s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T02:56:42.360031Z","iopub.execute_input":"2025-04-16T02:56:42.360703Z","iopub.status.idle":"2025-04-16T02:56:43.954240Z","shell.execute_reply.started":"2025-04-16T02:56:42.360680Z","shell.execute_reply":"2025-04-16T02:56:43.953662Z"}},"outputs":[{"name":"stdout","text":"Top 5 suggestions:  ['à®®à®²à¯ˆ', 'à®®à®¾à®²à¯ˆ', 'à®®à®´à¯ˆ', 'à®®à®²à®°à¯', 'à®®à®²à¯ˆà®ªà¯']\nFinal score of suggestions:  {'à®®à®²à¯ˆ': 0.388294858961861, 'à®®à®²à®®à¯': 0.11510631441280753, 'à®®à®¾à®²à¯ˆ': 0.15644210731162336, 'à®®à®´à¯ˆ': 0.14845699724712824, 'à®®à®²à®°à¯': 0.1273564013395307, 'à®®à®²à¯ˆà®šà¯': 0.10684886671622212, 'à®®à®²à¯ˆà®ªà¯': 0.12429111241342154, 'à®®à®²à¯ˆà®¤à¯': 0.12215819654059698, 'à®®à®²à¯ˆà®•à¯': 0.10195928738847512, 'à®®à®²à®¾à®¯à¯': 0.0555532014270788, 'à®²à®µà¯': 0.04391690634448177}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"l,_ = get_corrections_stat(\"à®“à®Ÿà®¿à®©à®¾à®°à¯\", probs,vocab,\"à®‡à®™à¯à®•à¯‡\",\"à®…à®µà®°à¯à®•à®³à¯\")\nprint(\"Top 10 suggestions: \", l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T02:56:53.532569Z","iopub.execute_input":"2025-04-16T02:56:53.533213Z","iopub.status.idle":"2025-04-16T02:56:53.663994Z","shell.execute_reply.started":"2025-04-16T02:56:53.533192Z","shell.execute_reply":"2025-04-16T02:56:53.663215Z"}},"outputs":[{"name":"stdout","text":"Top 10 suggestions:  ['à®“à®°à¯', 'à®“à®Ÿà®¿', 'à®“à®Ÿà®¿à®©à®¾à®°à¯', 'à®“à®Ÿà®¿à®©à®°à¯', 'à®“à®Ÿà®¿à®©', 'à®“à®Ÿà®¿à®©à®¾à®³à¯', 'à®“à®Ÿà®¿à®©à®¾à®°à¯à®•à®³à¯', 'à®“à®©à®¾', 'à®“à®Ÿà®¿à®•à®³à¯', 'à®“à®©à®°à¯']\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"l,_,s = (get_corrections(\"à®“à®Ÿà®¿à®©à®¾à®°à¯\", 2, \"à®…à®µà®°à¯à®•à®³à¯ à®‡à®™à¯à®•à¯‡ à®“à®Ÿà®¿à®©à®¾à®°à¯\", probs,vocab,\"à®‡à®™à¯à®•à¯‡\",\"à®…à®µà®°à¯à®•à®³à¯\"))\nprint(\"Top 5 suggestions: \", l)\nprint(\"Final score of suggestions: \", s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T02:56:59.437049Z","iopub.execute_input":"2025-04-16T02:56:59.437827Z","iopub.status.idle":"2025-04-16T02:57:01.186230Z","shell.execute_reply.started":"2025-04-16T02:56:59.437802Z","shell.execute_reply":"2025-04-16T02:57:01.185646Z"}},"outputs":[{"name":"stdout","text":"Top 5 suggestions:  ['à®“à®Ÿà®¿à®©à®¾à®°à¯', 'à®“à®Ÿà®¿à®©à®°à¯', 'à®“à®Ÿà®¿à®©à®¾à®³à¯', 'à®“à®°à¯', 'à®’à®Ÿà®¿à®©à®¾à®°à¯']\nFinal score of suggestions:  {'à®“à®Ÿà®¿à®©à®¾à®°à¯': 0.3091906113677708, 'à®“à®Ÿà®¿à®©à®¾à®³à¯': 0.10475238115491146, 'à®’à®Ÿà®¿à®©à®¾à®°à¯': 0.10163176995688969, 'à®“à®Ÿà®¿à®©à®¾à®°à¯': 0.10089536935699328, 'à®“à®Ÿà®¿à®©à®°à¯': 0.11603635410213195, 'à®“à®Ÿà®¿à®©à®¾à®°à¯‡': 0.0936191579981198, 'à®“à®Ÿà®¿à®©à®¾à®°à¯‹': 0.08973941821564556, 'à®“à®Ÿà®¿à®©à®¾à®°à®¾à®®à¯': 0.03932567683177029, 'à®“à®Ÿà®¿à®©': 0.03179872861172543, 'à®“à®Ÿà®¿à®•à®³à¯': 0.01126184415426012, 'à®“à®©à®°à¯': 0.011065667810320664, 'à®“à®Ÿà®¿à®©à®¾à®°à¯à®•à®³à¯': 0.01547948288328875, 'à®“à®©à®¾': 0.008003057106680346, 'à®“à®°à¯': 0.10435875189082768, 'à®“à®Ÿà®¿': 0.05371313056179184}\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"l,_ = get_corrections_stat(\"à®¤à¯†à®³à¯€à®µà¯\", probs,vocab,\"<s> \",\"<s>\")\nprint(\"Top 10 suggestions: \", l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T02:57:06.758798Z","iopub.execute_input":"2025-04-16T02:57:06.759154Z","iopub.status.idle":"2025-04-16T02:57:06.874198Z","shell.execute_reply.started":"2025-04-16T02:57:06.759126Z","shell.execute_reply":"2025-04-16T02:57:06.873405Z"}},"outputs":[{"name":"stdout","text":"Top 10 suggestions:  ['à®¤à¯†à®³à®¿à®µà¯', 'à®¤à¯†à®³à®¿à®µà¯à®®à¯', 'à®¤à¯†à®³à®¿', 'à®¤à¯†à®³à®¿à®µà¯à®¤à¯', 'à®¤à¯‡à®µà¯', 'à®¤à¯†à®³à®¿à®µà¯à®ªà¯', 'à®¤à¯†à®³à¯€à®µà¯']\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"l,_,s = (get_corrections(\"à®¤à¯†à®³à¯€à®µà¯\", 2, \"à®¤à¯†à®³à¯€à®µà¯ à®®à®¿à®• à®…à®µà®šà®¿à®¯à®®à¯\", probs,vocab,\"<s>\",\"<s>\"))\nprint(\"Top 5 suggestions: \", l)\nprint(\"Final score of suggestions: \", s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T02:57:12.291114Z","iopub.execute_input":"2025-04-16T02:57:12.291535Z","iopub.status.idle":"2025-04-16T02:57:13.663209Z","shell.execute_reply.started":"2025-04-16T02:57:12.291424Z","shell.execute_reply":"2025-04-16T02:57:13.662588Z"}},"outputs":[{"name":"stdout","text":"Top 5 suggestions:  ['à®¤à¯†à®³à®¿à®µà¯', 'à®¤à¯†à®³à®¿à®µà¯à®®à¯', 'à®¤à¯†à®³à®¿', 'à®¤à¯†à®³à®¿à®µà¯à®ªà¯', 'à®¤à¯‡à®µà¯']\nFinal score of suggestions:  {'à®¤à¯†à®³à®¿à®µà¯': 0.6271821613924031, 'à®¤à¯†à®³à®¿à®µà¯à®ªà¯': 0.08152176129401367, 'à®¤à¯‡à®µà¯': 0.08117326118449035, 'à®¤à¯†à®³à®¿à®µà¯à®®à¯': 0.12660437206820588, 'à®¤à¯†à®³à®¿': 0.11418255494891777, 'à®¤à¯†à®³à®¿à®µà¯à®¤à¯': 0.059956911673322434}\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"l,_ = get_corrections_stat(\"à®‡à®°à®Ÿà¯à®Ÿà¯ˆà®•à®¿à®³à®µà®¿\", probs,vocab,\"à®à®©à¯à®ªà®¤à¯\",\"à®ªà®²à®ªà®²\")\nprint(\"Top 10 suggestions: \", l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T02:57:20.709121Z","iopub.execute_input":"2025-04-16T02:57:20.709816Z","iopub.status.idle":"2025-04-16T02:57:20.733252Z","shell.execute_reply.started":"2025-04-16T02:57:20.709791Z","shell.execute_reply":"2025-04-16T02:57:20.732596Z"}},"outputs":[{"name":"stdout","text":"Top 10 suggestions:  ['à®‡à®°à®Ÿà¯à®Ÿà¯ˆà®•à®¿à®³à®µà®¿']\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"l,_,s = (get_corrections(\"à®‡à®°à®Ÿà¯à®Ÿà¯ˆà®•à®¿à®³à®µà®¿\", 2, \"à®ªà®²à®ªà®² à®à®©à¯à®ªà®¤à¯ à®‡à®°à®Ÿà¯à®Ÿà¯ˆà®•à®¿à®³à®µà®¿\",probs,vocab,\"à®à®©à¯à®ªà®¤à¯\",\"à®ªà®²à®ªà®²\"))\nprint(\"Top 5 suggestions: \", l)\nprint(\"Final score of suggestions: \", s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T02:57:26.389198Z","iopub.execute_input":"2025-04-16T02:57:26.389772Z","iopub.status.idle":"2025-04-16T02:57:27.483099Z","shell.execute_reply.started":"2025-04-16T02:57:26.389751Z","shell.execute_reply":"2025-04-16T02:57:27.482308Z"}},"outputs":[{"name":"stdout","text":"Top 5 suggestions:  ['à®‡à®°à®Ÿà¯à®Ÿà¯ˆà®•à¯à®•à®¿à®³à®µà®¿', 'à®‡à®°à®Ÿà¯à®Ÿà¯ˆà®®à¯Šà®´à®¿']\nFinal score of suggestions:  {'à®‡à®°à®Ÿà¯à®Ÿà¯ˆà®•à¯à®•à®¿à®³à®µà®¿': 0.5246624513610008, 'à®‡à®°à®Ÿà¯à®Ÿà¯ˆà®®à¯Šà®´à®¿': 0.4753375486389992}\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"l,_ = get_corrections_stat(\"à®’à®¯à®¿à®²à®¾à®Ÿà¯à®Ÿà®®\", probs,vocab,\"à®•à®²à¯ˆà®µà®Ÿà®¿à®µà®®à¯\",\"à®¤à®®à®¿à®´à®•\")\nprint(\"Top 10 suggestions: \", l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:17:41.381333Z","iopub.execute_input":"2025-04-15T15:17:41.382061Z","iopub.status.idle":"2025-04-15T15:17:41.419685Z","shell.execute_reply.started":"2025-04-15T15:17:41.382001Z","shell.execute_reply":"2025-04-15T15:17:41.418944Z"}},"outputs":[{"name":"stdout","text":"Top 10 suggestions:  ['à®’à®¯à®¿à®²à®¾à®Ÿà¯à®Ÿà®®à¯', 'à®’à®¯à®¿à®²à®¾à®Ÿà¯à®Ÿà®®']\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"l,_,s = (get_corrections(\"à®’à®¯à®¿à®²à®¾à®Ÿà¯à®Ÿà®®\", 3, \"à®ªà®£à¯à®Ÿà¯ˆà®¯ à®¤à®®à®¿à®´à®• à®•à®²à¯ˆà®µà®Ÿà®¿à®µà®®à¯ à®’à®¯à®¿à®²à®¾à®Ÿà¯à®Ÿà®®\", probs,vocab,\"à®•à®²à¯ˆà®µà®Ÿà®¿à®µà®®à¯\",\"à®¤à®®à®¿à®´à®•\"))\nprint(\"Top 5 suggestions: \", l)\nprint(\"Final score of suggestions: \", s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T02:57:42.903226Z","iopub.execute_input":"2025-04-16T02:57:42.903546Z","iopub.status.idle":"2025-04-16T02:57:44.140631Z","shell.execute_reply.started":"2025-04-16T02:57:42.903523Z","shell.execute_reply":"2025-04-16T02:57:44.140002Z"}},"outputs":[{"name":"stdout","text":"Top 5 suggestions:  ['à®’à®¯à®¿à®²à®¾à®Ÿà¯à®Ÿà®®à¯', 'à®’à®¯à®¿à®²à®¾à®Ÿà¯à®Ÿ', 'à®Šà®Ÿà®¾à®Ÿà¯à®Ÿà®®', 'à®¨à®¾à®Ÿà¯à®Ÿà®®']\nFinal score of suggestions:  {'à®’à®¯à®¿à®²à®¾à®Ÿà¯à®Ÿ': 0.48081511638996605, 'à®’à®¯à®¿à®²à®¾à®Ÿà¯à®Ÿà®®à¯': 0.5280108257694286, 'à®Šà®Ÿà®¾à®Ÿà¯à®Ÿà®®': 0.02228871129977663, 'à®¨à®¾à®Ÿà¯à®Ÿà®®': 0.021328588122003316}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"l,_ = get_corrections_stat(\"à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®•\", probs,vocab,\"à®¨à¯‹à®•à¯à®•à®¿à®ªà¯\",\"à®¤à®©à¯à®©à¯ˆ\")\nprint(\"Top 10 suggestions: \", l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T03:06:25.796074Z","iopub.execute_input":"2025-04-16T03:06:25.796469Z","iopub.status.idle":"2025-04-16T03:06:25.943949Z","shell.execute_reply.started":"2025-04-16T03:06:25.796420Z","shell.execute_reply":"2025-04-16T03:06:25.943177Z"}},"outputs":[{"name":"stdout","text":"Top 10 suggestions:  ['à®ªà®¾à®°à¯à®¤à¯à®¤', 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®•', 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®•à®•à¯', 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à¯ˆ', 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®•à®šà¯', 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®•à®ªà¯', 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¾', 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®•à®¤à¯', 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¾à®•']\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"l,_,s = (get_corrections(\"à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®•\", 2, \"à®…à®µà®©à¯ à®¤à®©à¯à®©à¯ˆ à®¨à¯‹à®•à¯à®•à®¿à®ªà¯ à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®• à®•à®°à¯à®¤à®¿à®©à®¾à®©à¯\", probs,vocab,\"à®¨à¯‹à®•à¯à®•à®¿à®ªà¯\",\"à®¤à®©à¯à®©à¯ˆ\"))\nprint(\"Top 5 suggestions: \", l)\nprint(\"Final score of suggestions: \", s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T03:09:33.330203Z","iopub.execute_input":"2025-04-16T03:09:33.330463Z","iopub.status.idle":"2025-04-16T03:09:35.031072Z","shell.execute_reply.started":"2025-04-16T03:09:33.330427Z","shell.execute_reply":"2025-04-16T03:09:35.030296Z"}},"outputs":[{"name":"stdout","text":"Top 5 suggestions:  ['à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®•', 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¾à®•', 'à®ªà®¾à®°à¯à®¤à¯à®¤', 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®•à®•à¯', 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à¯ˆ']\nFinal score of suggestions:  {'à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®•': 0.4068465097923238, 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¾à®•': 0.1292054660302927, 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¾': 0.06230724661299769, 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®•à®šà¯': 0.06561519756143865, 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®•à®ªà¯': 0.06553543959578581, 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®•à®¤à¯': 0.055184008988141166, 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®•à®•à¯': 0.0799876241923697, 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à¯ˆ': 0.07328140118442363, 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®¯à¯': 0.05116931604889675, 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®•à®µà¯‡': 0.048769835345418615, 'à®ªà®¾à®°à¯à®¤à¯à®¤': 0.11806352512564959, 'à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®•à®µà¯à®®à¯': 0.007268228972506433}\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_name = \"abhinand/tamil-llama-7b-instruct-v0.1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.float16  # useful for Kaggle memory constraints\n)\ndef correct_with_llm(word, sentence):\n    prompt = f\"\"\"\n    ### Instruction:\n    à®ªà®¿à®©à¯à®µà®°à¯à®®à¯ à®µà®¾à®•à¯à®•à®¿à®¯à®¤à¯à®¤à®¿à®²à¯ à®‰à®³à¯à®³ à®à®´à¯à®¤à¯à®¤à¯à®ªà¯ à®ªà®¿à®´à¯ˆà®•à®³à¯ˆ à®šà®°à®¿à®šà¯†à®¯à¯à®¯à®µà¯à®®à¯.\n    \n    ### Input:\n    {sentence}\n    \n    ### Response:\n    \"\"\".strip()\n\n#     prompt = f\"\"\"\n# ### Instruction:\n# à®•à¯€à®´à¯‡ à®•à¯Šà®Ÿà¯à®•à¯à®•à®ªà¯à®ªà®Ÿà¯à®Ÿà¯à®³à¯à®³ à®ªà®¿à®´à¯ˆà®¯à®¾à®© à®šà¯Šà®²à¯à®²à¯à®•à¯à®•à®¾à®© à®šà®°à®¿à®¯à®¾à®© 10 à®ªà®¤à®¿à®²à¯à®•à®³à¯ˆ à®µà®°à®¿à®šà¯ˆà®ªà¯à®ªà®Ÿà¯à®¤à¯à®¤à®¿ à®¤à®°à®µà¯à®®à¯.\n\n# ### Input:\n# à®µà®¾à®•à¯à®•à®¿à®¯à®®à¯: {sentence}\n# à®ªà®¿à®´à¯ˆà®¯à®¾à®© à®šà¯Šà®²à¯: {word}\n\n# ### Response:\n# \"\"\".strip()\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=256,\n            temperature=0.5,\n            top_p=0.9,\n            do_sample=True,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n\n    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n    result = decoded.split(\"### Response:\")[-1].strip()\n    return decoded\n\nllm_out = correct_with_llm(\"à®ªà®²à®®à¯\",\"à®…à®µà®©à¯ à®ªà®²à®®à¯ à®šà®¾à®ªà¯à®ªà®¿à®Ÿà¯à®Ÿà®¾à®©à¯\")\nprint(llm_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T13:31:11.825386Z","iopub.execute_input":"2025-05-04T13:31:11.825976Z","iopub.status.idle":"2025-05-04T13:32:50.398558Z","shell.execute_reply.started":"2025-05-04T13:31:11.825939Z","shell.execute_reply":"2025-05-04T13:32:50.397685Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f32089ff8f0498da1e6776624bf302f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab37dc6283f84b5f85a21d98371d1fa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00007-of-00007.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c4507a1b7af47cebe0e7b12a571cf8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00007.bin:   0%|          | 0.00/1.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8929edb9062a4ffabdb1186934b7c897"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82ff505243bb46829b2d299c192aa99d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5d73e2844bf471f9e95ddccbfeea687"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00005-of-00007.bin:   0%|          | 0.00/1.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b6a2962d80c4b0fb0836df22ede8d6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00006-of-00007.bin:   0%|          | 0.00/1.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c58c35a53234ee9afa5e1d99c7962a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/28.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bda9fee3cac4d18b70dd67b193c0f3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48bef4244a944160a12987939e838ffd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6def7eed6d7414a83528cbb5e1674bf"}},"metadata":{}},{"name":"stdout","text":"### Instruction:\n    à®ªà®¿à®©à¯à®µà®°à¯à®®à¯ à®µà®¾à®•à¯à®•à®¿à®¯à®¤à¯à®¤à®¿à®²à¯ à®‰à®³à¯à®³ à®à®´à¯à®¤à¯à®¤à¯à®ªà¯ à®ªà®¿à®´à¯ˆà®•à®³à¯ˆ à®šà®°à®¿à®šà¯†à®¯à¯à®¯à®µà¯à®®à¯.\n    \n    ### Input:\n    à®…à®µà®©à¯ à®ªà®²à®®à¯ à®šà®¾à®ªà¯à®ªà®¿à®Ÿà¯à®Ÿà®¾à®©à¯\n    \n    ### Response:\n    à®…à®µà®©à¯ à®µà®²à®¿à®®à¯ˆà®¯à¯à®Ÿà®©à¯ à®šà®¾à®ªà¯à®ªà®¿à®Ÿà¯à®Ÿà®¾à®©à¯.\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"llm_out = correct_with_llm(\"à®‡à®°à®Ÿà¯à®Ÿà¯ˆà®•à®¿à®³à®µà®¿\",\"à®ªà®²à®ªà®² à®à®©à¯à®ªà®¤à¯ à®‡à®°à®Ÿà¯à®Ÿà¯ˆà®•à®¿à®³à®µà®¿\")\nprint(llm_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T13:34:15.207992Z","iopub.execute_input":"2025-05-04T13:34:15.208272Z","iopub.status.idle":"2025-05-04T13:34:16.126260Z","shell.execute_reply.started":"2025-05-04T13:34:15.208252Z","shell.execute_reply":"2025-05-04T13:34:16.125513Z"}},"outputs":[{"name":"stdout","text":"### Instruction:\n    à®ªà®¿à®©à¯à®µà®°à¯à®®à¯ à®µà®¾à®•à¯à®•à®¿à®¯à®¤à¯à®¤à®¿à®²à¯ à®‰à®³à¯à®³ à®à®´à¯à®¤à¯à®¤à¯à®ªà¯ à®ªà®¿à®´à¯ˆà®•à®³à¯ˆ à®šà®°à®¿à®šà¯†à®¯à¯à®¯à®µà¯à®®à¯.\n    \n    ### Input:\n    à®ªà®²à®ªà®² à®à®©à¯à®ªà®¤à¯ à®‡à®°à®Ÿà¯à®Ÿà¯ˆà®•à®¿à®³à®µà®¿\n    \n    ### Response:\n    à®ªà®²à®ªà®² à®à®©à¯à®ªà®¤à¯ à®‡à®°à®Ÿà¯à®Ÿà¯ˆà®•à¯ à®•à¯à®Ÿà¯à®®à¯à®ªà®®à®¾à®•à¯à®®à¯.\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"llm_out = correct_with_llm(\"à®¤à¯†à®³à¯€à®µà¯\",\"à®¤à¯†à®³à¯€à®µà¯ à®®à®¿à®• à®…à®µà®šà®¿à®¯à®®à¯\")\nprint(llm_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T13:36:17.405247Z","iopub.execute_input":"2025-05-04T13:36:17.405520Z","iopub.status.idle":"2025-05-04T13:36:18.265532Z","shell.execute_reply.started":"2025-05-04T13:36:17.405499Z","shell.execute_reply":"2025-05-04T13:36:18.264759Z"}},"outputs":[{"name":"stdout","text":"### Instruction:\n    à®ªà®¿à®©à¯à®µà®°à¯à®®à¯ à®µà®¾à®•à¯à®•à®¿à®¯à®¤à¯à®¤à®¿à®²à¯ à®‰à®³à¯à®³ à®à®´à¯à®¤à¯à®¤à¯à®ªà¯ à®ªà®¿à®´à¯ˆà®•à®³à¯ˆ à®šà®°à®¿à®šà¯†à®¯à¯à®¯à®µà¯à®®à¯.\n    \n    ### Input:\n    à®¤à¯†à®³à¯€à®µà¯ à®®à®¿à®• à®…à®µà®šà®¿à®¯à®®à¯\n    \n    ### Response:\n    à®¤à¯†à®³à¯‚à®°à®¿ à®®à®¿à®• à®®à¯à®•à¯à®•à®¿à®¯à®®à®¾à®©à®¤à®¾à®• à®‡à®°à¯à®¨à¯à®¤à®¤à¯.\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"llm_out = correct_with_llm(\"à®®à®²\",\"à®…à®¤à¯ à®ªà¯†à®°à®¿à®¯ à®®à®²\")\nprint(llm_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T13:36:45.538426Z","iopub.execute_input":"2025-05-04T13:36:45.539010Z","iopub.status.idle":"2025-05-04T13:36:46.152325Z","shell.execute_reply.started":"2025-05-04T13:36:45.538977Z","shell.execute_reply":"2025-05-04T13:36:46.151756Z"}},"outputs":[{"name":"stdout","text":"### Instruction:\n    à®ªà®¿à®©à¯à®µà®°à¯à®®à¯ à®µà®¾à®•à¯à®•à®¿à®¯à®¤à¯à®¤à®¿à®²à¯ à®‰à®³à¯à®³ à®à®´à¯à®¤à¯à®¤à¯à®ªà¯ à®ªà®¿à®´à¯ˆà®•à®³à¯ˆ à®šà®°à®¿à®šà¯†à®¯à¯à®¯à®µà¯à®®à¯.\n    \n    ### Input:\n    à®…à®¤à¯ à®ªà¯†à®°à®¿à®¯ à®®à®²\n    \n    ### Response:\n    à®…à®¤à¯ à®’à®°à¯ à®ªà¯†à®°à®¿à®¯ à®¨à®¾à®¯à¯.\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"llm_out = correct_with_llm(\"à®’à®¯à®¿à®²à®¾à®Ÿà¯à®Ÿà®®\",\"à®ªà®£à¯à®Ÿà¯ˆà®¯ à®¤à®®à®¿à®´à®• à®•à®²à¯ˆà®µà®Ÿà®¿à®µà®®à¯ à®’à®¯à®¿à®²à®¾à®Ÿà¯à®Ÿà®®\")\nprint(llm_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T13:37:28.754341Z","iopub.execute_input":"2025-05-04T13:37:28.754806Z","iopub.status.idle":"2025-05-04T13:37:29.796224Z","shell.execute_reply.started":"2025-05-04T13:37:28.754782Z","shell.execute_reply":"2025-05-04T13:37:29.795587Z"}},"outputs":[{"name":"stdout","text":"### Instruction:\n    à®ªà®¿à®©à¯à®µà®°à¯à®®à¯ à®µà®¾à®•à¯à®•à®¿à®¯à®¤à¯à®¤à®¿à®²à¯ à®‰à®³à¯à®³ à®à®´à¯à®¤à¯à®¤à¯à®ªà¯ à®ªà®¿à®´à¯ˆà®•à®³à¯ˆ à®šà®°à®¿à®šà¯†à®¯à¯à®¯à®µà¯à®®à¯.\n    \n    ### Input:\n    à®ªà®£à¯à®Ÿà¯ˆà®¯ à®¤à®®à®¿à®´à®• à®•à®²à¯ˆà®µà®Ÿà®¿à®µà®®à¯ à®’à®¯à®¿à®²à®¾à®Ÿà¯à®Ÿà®®\n    \n    ### Response:\n    à®ªà®£à®¿à®¯à®¿à®Ÿ à®•à®²à¯ˆà®µà®Ÿà®¿à®µà®®à¯ à®’à®¯à®¿à®²à®¾à®Ÿà¯à®Ÿà®®\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"llm_out = correct_with_llm(\"à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®©à¯\",\"à®…à®µà®³à¯ à®ªà¯à®¤à¯à®¤à®•à®®à¯ à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®©à¯\")\nprint(llm_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T13:38:55.880261Z","iopub.execute_input":"2025-05-04T13:38:55.880900Z","iopub.status.idle":"2025-05-04T13:38:56.494350Z","shell.execute_reply.started":"2025-05-04T13:38:55.880877Z","shell.execute_reply":"2025-05-04T13:38:56.493778Z"}},"outputs":[{"name":"stdout","text":"### Instruction:\n    à®ªà®¿à®©à¯à®µà®°à¯à®®à¯ à®µà®¾à®•à¯à®•à®¿à®¯à®¤à¯à®¤à®¿à®²à¯ à®‰à®³à¯à®³ à®à®´à¯à®¤à¯à®¤à¯à®ªà¯ à®ªà®¿à®´à¯ˆà®•à®³à¯ˆ à®šà®°à®¿à®šà¯†à®¯à¯à®¯à®µà¯à®®à¯.\n    \n    ### Input:\n    à®…à®µà®³à¯ à®ªà¯à®¤à¯à®¤à®•à®®à¯ à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®©à¯\n    \n    ### Response:\n    à®…à®µà®³à¯ à®ªà¯à®¤à¯à®¤à®•à®®à¯ à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®³à¯.\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"llm_out = correct_with_llm(\"à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®•\",\"à®…à®µà®©à¯ à®¤à®©à¯à®©à¯ˆ à®¨à¯‹à®•à¯à®•à®¿à®ªà¯ à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®• à®•à®°à¯à®¤à®¿à®©à®¾à®©à¯\")\nprint(llm_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T13:39:38.224805Z","iopub.execute_input":"2025-05-04T13:39:38.225517Z","iopub.status.idle":"2025-05-04T13:39:39.093277Z","shell.execute_reply.started":"2025-05-04T13:39:38.225483Z","shell.execute_reply":"2025-05-04T13:39:39.092535Z"}},"outputs":[{"name":"stdout","text":"### Instruction:\n    à®ªà®¿à®©à¯à®µà®°à¯à®®à¯ à®µà®¾à®•à¯à®•à®¿à®¯à®¤à¯à®¤à®¿à®²à¯ à®‰à®³à¯à®³ à®à®´à¯à®¤à¯à®¤à¯à®ªà¯ à®ªà®¿à®´à¯ˆà®•à®³à¯ˆ à®šà®°à®¿à®šà¯†à®¯à¯à®¯à®µà¯à®®à¯.\n    \n    ### Input:\n    à®…à®µà®©à¯ à®¤à®©à¯à®©à¯ˆ à®¨à¯‹à®•à¯à®•à®¿à®ªà¯ à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®• à®•à®°à¯à®¤à®¿à®©à®¾à®©à¯\n    \n    ### Response:\n    à®…à®µà®°à¯ à®¤à®©à¯à®©à¯ˆ à®¨à¯‹à®•à¯à®•à®¿à®ªà¯ à®ªà®¾à®°à¯à®¤à¯à®¤à®¤à®¾à®• à®•à®°à¯à®¤à®¿à®©à®¾à®°à¯.\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"llm_out = correct_with_llm(\"à®“à®Ÿà®¿à®©à®¾à®°à¯\",\"à®…à®µà®°à¯à®•à®³à¯ à®‡à®™à¯à®•à¯‡ à®“à®Ÿà®¿à®©à®¾à®°à¯\")\nprint(llm_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T13:41:00.950671Z","iopub.execute_input":"2025-05-04T13:41:00.951337Z","iopub.status.idle":"2025-05-04T13:41:01.659283Z","shell.execute_reply.started":"2025-05-04T13:41:00.951313Z","shell.execute_reply":"2025-05-04T13:41:01.658402Z"}},"outputs":[{"name":"stdout","text":"### Instruction:\n    à®ªà®¿à®©à¯à®µà®°à¯à®®à¯ à®µà®¾à®•à¯à®•à®¿à®¯à®¤à¯à®¤à®¿à®²à¯ à®‰à®³à¯à®³ à®à®´à¯à®¤à¯à®¤à¯à®ªà¯ à®ªà®¿à®´à¯ˆà®•à®³à¯ˆ à®šà®°à®¿à®šà¯†à®¯à¯à®¯à®µà¯à®®à¯.\n    \n    ### Input:\n    à®…à®µà®°à¯à®•à®³à¯ à®‡à®™à¯à®•à¯‡ à®“à®Ÿà®¿à®©à®¾à®°à¯\n    \n    ### Response:\n    à®…à®µà®°à¯à®•à®³à¯ à®‡à®™à¯à®•à¯‡ à®“à®Ÿà®¿à®©à®¾à®°à¯à®•à®³à¯.\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"llm_out = correct_with_llm(\"à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®©à¯\",\"à®…à®µà®°à¯ à®ªà¯à®¤à¯à®¤à®•à®®à¯ à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®©à¯\")\nprint(llm_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T13:41:43.837271Z","iopub.execute_input":"2025-05-04T13:41:43.837749Z","iopub.status.idle":"2025-05-04T13:41:44.449587Z","shell.execute_reply.started":"2025-05-04T13:41:43.837725Z","shell.execute_reply":"2025-05-04T13:41:44.448999Z"}},"outputs":[{"name":"stdout","text":"### Instruction:\n    à®ªà®¿à®©à¯à®µà®°à¯à®®à¯ à®µà®¾à®•à¯à®•à®¿à®¯à®¤à¯à®¤à®¿à®²à¯ à®‰à®³à¯à®³ à®à®´à¯à®¤à¯à®¤à¯à®ªà¯ à®ªà®¿à®´à¯ˆà®•à®³à¯ˆ à®šà®°à®¿à®šà¯†à®¯à¯à®¯à®µà¯à®®à¯.\n    \n    ### Input:\n    à®…à®µà®°à¯ à®ªà¯à®¤à¯à®¤à®•à®®à¯ à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®©à¯\n    \n    ### Response:\n    à®…à®µà®°à¯ à®ªà¯à®¤à¯à®¤à®•à®®à¯ à®ªà®Ÿà®¿à®¤à¯à®¤à®¾à®°à¯.\n\n","output_type":"stream"}],"execution_count":10}]}